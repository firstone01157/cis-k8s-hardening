master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ # 1. ดูรายการ ID ของตัวที่ตาย                                                                              ล่าสุด (ดูบรรทัดบนสุดที่สถานะ Exited)
sudo crictl ps -a | grep kube-apiserver | head -n 3

# 2. เอา Container ID ตัวที่ Exited (ตัวบนสุด) มาใส่แทน XXXXX
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/cric                                                                              tl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/cont                                                                              ainerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the                                                                               default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/contai                                                                              nerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the d                                                                              efault settings are now deprecated, you should set the endpoint instead.
275aec38f3bc9       a5f569d49a979       8 minutes ago       Exited                                                                                            kube-apiserver            9                   0d089d5dafed7       kube-apiserver                                                                              -terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ sudo crictl logs 275aec38                                                                              f3bc9 --tail 50
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/cric                                                                              tl.yaml"
NAME:
   crictl logs - Fetch the logs of a container

USAGE:
   crictl logs [command options] CONTAINER-ID

COMMANDS:
   help, h  Shows a list of commands or help for one command

OPTIONS:
   --follow, -f              Follow log output (default: false)
   --limit-bytes value       Maximum bytes of logs to return. Defaults to no lim                                                                              it (default: -1)
   --previous, -p            Print the logs for the previous instance of the con                                                                              tainer in a pod if it exists (default: false)
   --reopen, -r              Reopen the container logs for the provided containe                                                                              r (default: false)
   --since value             Show logs since timestamp (e.g. 2013-01-02T13:23:37                                                                              ) or relative (e.g. 42m for 42 minutes)
   --stream value, -s value  Show specified stream (stdout or stderr). Defaults                                                                               to both.
   --tail value              Number of lines to show from the end of the logs. D                                                                              efaults to all (default: -1)
   --timestamps, -t          Show timestamps (default: false)
   --help, -h                show help
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ sudo crictl logs 275aec38f3bc9
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
I1126 07:26:19.406920       1 options.go:263] external host was not specified, using 192.168.150.131
I1126 07:26:19.411477       1 server.go:150] Version: v1.34.2
I1126 07:26:19.411576       1 server.go:152] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
W1126 07:26:19.681208       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=certificates.k8s.io/v1alpha1
W1126 07:26:19.681252       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=admissionregistration.k8s.io/v1alpha1
W1126 07:26:19.681262       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=internal.apiserver.k8s.io/v1alpha1
W1126 07:26:19.681266       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=coordination.k8s.io/v1alpha2
W1126 07:26:19.681269       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=resource.k8s.io/v1alpha3
W1126 07:26:19.681272       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=scheduling.k8s.io/v1alpha1
W1126 07:26:19.681274       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storage.k8s.io/v1alpha1
W1126 07:26:19.681277       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=imagepolicy.k8s.io/v1alpha1
W1126 07:26:19.681280       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=rbac.authorization.k8s.io/v1alpha1
W1126 07:26:19.681283       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storagemigration.k8s.io/v1alpha1
W1126 07:26:19.681285       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=authentication.k8s.io/v1alpha1
W1126 07:26:19.681288       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=node.k8s.io/v1alpha1
W1126 07:26:19.694181       1 logging.go:55] [core] [Channel #1 SubChannel #2]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 07:26:19.695530       1 shared_informer.go:349] "Waiting for caches to sync" controller="node_authorizer"
W1126 07:26:19.695589       1 logging.go:55] [core] [Channel #4 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 07:26:19.703615       1 shared_informer.go:349] "Waiting for caches to sync" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1126 07:26:19.710441       1 plugins.go:157] Loaded 15 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,AlwaysPullImages,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,PodTopologyLabels,MutatingAdmissionPolicy,MutatingAdmissionWebhook.
I1126 07:26:19.710485       1 plugins.go:160] Loaded 15 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,AlwaysPullImages,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,DenyServiceExternalIPs,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I1126 07:26:19.710813       1 instance.go:239] Using reconciler: lease
W1126 07:26:19.711883       1 logging.go:55] [core] [Channel #9 SubChannel #10]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.718263       1 logging.go:55] [core] [Channel #13 SubChannel #14]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.727878       1 logging.go:55] [core] [Channel #21 SubChannel #22]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 07:26:19.741226       1 handler.go:285] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W1126 07:26:19.741275       1 genericapiserver.go:784] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I1126 07:26:19.745208       1 cidrallocator.go:197] starting ServiceCIDR Allocator Controller
W1126 07:26:19.746022       1 logging.go:55] [core] [Channel #27 SubChannel #28]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 07:26:19.752053       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.758598       1 logging.go:55] [core] [Channel #35 SubChannel #36]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.766227       1 logging.go:55] [core] [Channel #39 SubChannel #40]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 07:26:19.773123       1 logging.go:55] [core] [Channel #43 SubChannel #44]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.780422       1 logging.go:55] [core] [Channel #47 SubChannel #48]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 07:26:19.787220       1 logging.go:55] [core] [Channel #51 SubChannel #52]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.793879       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 07:26:19.801834       1 logging.go:55] [core] [Channel #59 SubChannel #60]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.807827       1 logging.go:55] [core] [Channel #63 SubChannel #64]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.815866       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.826524       1 logging.go:55] [core] [Channel #71 SubChannel #72]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.835927       1 logging.go:55] [core] [Channel #75 SubChannel #76]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.844966       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.853541       1 logging.go:55] [core] [Channel #83 SubChannel #84]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 07:26:19.863006       1 logging.go:55] [core] [Channel #87 SubChannel #88]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.871126       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 07:26:19.892162       1 handler.go:285] Adding GroupVersion  v1 to ResourceManager
I1126 07:26:19.892412       1 apis.go:112] API group "internal.apiserver.k8s.io" is not enabled, skipping.
W1126 07:26:19.893930       1 logging.go:55] [core] [Channel #95 SubChannel #96]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.901576       1 logging.go:55] [core] [Channel #99 SubChannel #100]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.912353       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.921434       1 logging.go:55] [core] [Channel #107 SubChannel #108]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.930401       1 logging.go:55] [core] [Channel #111 SubChannel #112]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.939899       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.954515       1 logging.go:55] [core] [Channel #119 SubChannel #120]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.966122       1 logging.go:55] [core] [Channel #123 SubChannel #124]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.973011       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 07:26:19.980173       1 logging.go:55] [core] [Channel #131 SubChannel #132]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:19.986547       1 logging.go:55] [core] [Channel #135 SubChannel #136]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 07:26:19.995650       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.002604       1 logging.go:55] [core] [Channel #143 SubChannel #144]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.011358       1 logging.go:55] [core] [Channel #147 SubChannel #148]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.022134       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.029978       1 logging.go:55] [core] [Channel #155 SubChannel #156]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.039863       1 logging.go:55] [core] [Channel #159 SubChannel #160]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.052995       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.062482       1 logging.go:55] [core] [Channel #167 SubChannel #168]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.071445       1 logging.go:55] [core] [Channel #171 SubChannel #172]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.080836       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.089861       1 logging.go:55] [core] [Channel #179 SubChannel #180]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.100251       1 logging.go:55] [core] [Channel #183 SubChannel #184]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.108039       1 logging.go:55] [core] [Channel #187 SubChannel #188]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 07:26:20.116374       1 apis.go:112] API group "storagemigration.k8s.io" is not enabled, skipping.
W1126 07:26:20.118504       1 logging.go:55] [core] [Channel #191 SubChannel #192]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.127249       1 logging.go:55] [core] [Channel #195 SubChannel #196]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.135185       1 logging.go:55] [core] [Channel #199 SubChannel #200]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.142124       1 logging.go:55] [core] [Channel #203 SubChannel #204]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.149227       1 logging.go:55] [core] [Channel #207 SubChannel #208]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.157097       1 logging.go:55] [core] [Channel #211 SubChannel #212]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.163397       1 logging.go:55] [core] [Channel #215 SubChannel #216]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.171039       1 logging.go:55] [core] [Channel #219 SubChannel #220]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.179075       1 logging.go:55] [core] [Channel #223 SubChannel #224]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.185609       1 logging.go:55] [core] [Channel #227 SubChannel #228]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.194141       1 logging.go:55] [core] [Channel #231 SubChannel #232]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 07:26:20.201550       1 logging.go:55] [core] [Channel #235 SubChannel #236]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.208348       1 logging.go:55] [core] [Channel #239 SubChannel #240]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 07:26:20.228146       1 logging.go:55] [core] [Channel #243 SubChannel #244]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.236293       1 logging.go:55] [core] [Channel #247 SubChannel #248]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 07:26:20.242765       1 logging.go:55] [core] [Channel #251 SubChannel #252]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 07:26:20.266798       1 handler.go:285] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W1126 07:26:20.266844       1 genericapiserver.go:784] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W1126 07:26:20.266852       1 genericapiserver.go:784] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I1126 07:26:20.267166       1 handler.go:285] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W1126 07:26:20.267198       1 genericapiserver.go:784] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I1126 07:26:20.267723       1 handler.go:285] Adding GroupVersion autoscaling v2 to ResourceManager
I1126 07:26:20.268123       1 handler.go:285] Adding GroupVersion autoscaling v1 to ResourceManager
W1126 07:26:20.268154       1 genericapiserver.go:784] Skipping API autoscaling/v2beta1 because it has no resources.
W1126 07:26:20.268160       1 genericapiserver.go:784] Skipping API autoscaling/v2beta2 because it has no resources.
I1126 07:26:20.268876       1 handler.go:285] Adding GroupVersion batch v1 to ResourceManager
W1126 07:26:20.268910       1 genericapiserver.go:784] Skipping API batch/v1beta1 because it has no resources.
I1126 07:26:20.269419       1 handler.go:285] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W1126 07:26:20.269453       1 genericapiserver.go:784] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W1126 07:26:20.269459       1 genericapiserver.go:784] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I1126 07:26:20.269766       1 handler.go:285] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W1126 07:26:20.269796       1 genericapiserver.go:784] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W1126 07:26:20.269802       1 genericapiserver.go:784] Skipping API coordination.k8s.io/v1alpha2 because it has no resources.
I1126 07:26:20.270148       1 handler.go:285] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
W1126 07:26:20.270181       1 genericapiserver.go:784] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I1126 07:26:20.271636       1 handler.go:285] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W1126 07:26:20.271673       1 genericapiserver.go:784] Skipping API networking.k8s.io/v1beta1 because it has no resources.
I1126 07:26:20.271987       1 handler.go:285] Adding GroupVersion node.k8s.io v1 to ResourceManager
W1126 07:26:20.272022       1 genericapiserver.go:784] Skipping API node.k8s.io/v1beta1 because it has no resources.
W1126 07:26:20.272029       1 genericapiserver.go:784] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I1126 07:26:20.272526       1 handler.go:285] Adding GroupVersion policy v1 to ResourceManager
W1126 07:26:20.272561       1 genericapiserver.go:784] Skipping API policy/v1beta1 because it has no resources.
I1126 07:26:20.273672       1 handler.go:285] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W1126 07:26:20.273705       1 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W1126 07:26:20.273712       1 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I1126 07:26:20.274008       1 handler.go:285] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W1126 07:26:20.274040       1 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W1126 07:26:20.274047       1 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I1126 07:26:20.275388       1 handler.go:285] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W1126 07:26:20.275422       1 genericapiserver.go:784] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W1126 07:26:20.275428       1 genericapiserver.go:784] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I1126 07:26:20.276114       1 handler.go:285] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
W1126 07:26:20.276146       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta3 because it has no resources.
W1126 07:26:20.276153       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
W1126 07:26:20.276155       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
I1126 07:26:20.279001       1 handler.go:285] Adding GroupVersion apps v1 to ResourceManager
W1126 07:26:20.279039       1 genericapiserver.go:784] Skipping API apps/v1beta2 because it has no resources.
W1126 07:26:20.279046       1 genericapiserver.go:784] Skipping API apps/v1beta1 because it has no resources.
I1126 07:26:20.280522       1 handler.go:285] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W1126 07:26:20.280557       1 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W1126 07:26:20.280564       1 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I1126 07:26:20.281024       1 handler.go:285] Adding GroupVersion events.k8s.io v1 to ResourceManager
W1126 07:26:20.281098       1 genericapiserver.go:784] Skipping API events.k8s.io/v1beta1 because it has no resources.
W1126 07:26:20.281182       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta2 because it has no resources.
I1126 07:26:20.282742       1 handler.go:285] Adding GroupVersion resource.k8s.io v1 to ResourceManager
W1126 07:26:20.282776       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta1 because it has no resources.
W1126 07:26:20.282782       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1alpha3 because it has no resources.
W1126 07:26:20.285170       1 logging.go:55] [core] [Channel #255 SubChannel #256]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 07:26:20.291188       1 handler.go:285] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1126 07:26:20.291240       1 genericapiserver.go:784] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1126 07:26:20.597933       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 07:26:20.597933       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1126 07:26:20.598630       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/etc/kubernetes/pki/apiserver.crt::/etc/kubernetes/pki/apiserver.key"
I1126 07:26:20.598753       1 secure_serving.go:211] Serving securely on [::]:6443
I1126 07:26:20.598802       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1126 07:26:20.598995       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I1126 07:26:20.599103       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I1126 07:26:20.599139       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1126 07:26:20.599217       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/etc/kubernetes/pki/front-proxy-client.crt::/etc/kubernetes/pki/front-proxy-client.key"
I1126 07:26:20.599327       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1126 07:26:20.599406       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 07:26:20.599161       1 aggregator.go:169] waiting for initial CRD sync...
I1126 07:26:20.599171       1 controller.go:78] Starting OpenAPI AggregationController
I1126 07:26:20.599955       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1126 07:26:20.600119       1 repairip.go:210] Starting ipallocator-repair-controller
I1126 07:26:20.600249       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I1126 07:26:20.600988       1 controller.go:142] Starting OpenAPI controller
I1126 07:26:20.599180       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1126 07:26:20.601078       1 controller.go:90] Starting OpenAPI V3 controller
I1126 07:26:20.599198       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1126 07:26:20.599207       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1126 07:26:20.599278       1 controller.go:119] Starting legacy_token_tracking_controller
I1126 07:26:20.599296       1 local_available_controller.go:156] Starting LocalAvailability controller
I1126 07:26:20.599302       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1126 07:26:20.601218       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1126 07:26:20.601246       1 naming_controller.go:299] Starting NamingConditionController
I1126 07:26:20.601280       1 establishing_controller.go:81] Starting EstablishingController
I1126 07:26:20.601307       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1126 07:26:20.601336       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1126 07:26:20.601368       1 crd_finalizer.go:269] Starting CRDFinalizer
I1126 07:26:20.601429       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1126 07:26:20.601472       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
I1126 07:26:20.601553       1 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
I1126 07:26:20.601584       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I1126 07:26:20.600143       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I1126 07:26:20.628346       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1126 07:26:20.631866       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I1126 07:26:20.631981       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1126 07:26:20.695716       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1126 07:26:20.699325       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1126 07:26:20.701082       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1126 07:26:20.701401       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1126 07:26:20.701523       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1126 07:26:20.701706       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1126 07:26:20.701851       1 aggregator.go:171] initial CRD sync complete...
I1126 07:26:20.702013       1 autoregister_controller.go:144] Starting autoregister controller
I1126 07:26:20.702114       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1126 07:26:20.702167       1 cache.go:39] Caches are synced for autoregister controller
I1126 07:26:20.701901       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1126 07:26:20.704101       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1126 07:26:20.704309       1 policy_source.go:240] refreshing policies
I1126 07:26:20.705814       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1126 07:26:20.728794       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1126 07:26:20.732021       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1126 07:26:20.732078       1 cache.go:39] Caches are synced for LocalAvailability controller
I1126 07:26:20.732085       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1126 07:26:20.732096       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1126 07:26:20.746171       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1126 07:26:20.768915       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1126 07:26:21.603502       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W1126 07:26:21.911262       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.150.131]
I1126 07:26:21.912573       1 controller.go:667] quota admission added evaluator for: endpoints
I1126 07:26:21.917837       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1126 07:27:15.029807       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1126 07:30:19.147830       1 controller.go:128] Shutting down kubernetes service endpoint reconciler
W1126 07:30:19.164886       1 lease.go:265] Resetting endpoints for master service "kubernetes" to []
I1126 07:30:19.173475       1 object_count_tracker.go:141] "StorageObjectCountTracker pruner is exiting"
I1126 07:30:19.173986       1 naming_controller.go:310] Shutting down NamingConditionController
I1126 07:30:19.174521       1 dynamic_serving_content.go:149] "Shutting down controller" name="aggregator-proxy-cert::/etc/kubernetes/pki/front-proxy-client.crt::/etc/kubernetes/pki/front-proxy-client.key"
I1126 07:30:19.174563       1 controller.go:157] Shutting down quota evaluator
I1126 07:30:19.174702       1 controller.go:176] quota evaluator worker shutdown
I1126 07:30:19.174288       1 dynamic_cafile_content.go:175] "Shutting down controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1126 07:30:19.173995       1 establishing_controller.go:92] Shutting down EstablishingController
I1126 07:30:19.174411       1 dynamic_cafile_content.go:175] "Shutting down controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 07:30:19.173999       1 apiapproval_controller.go:201] Shutting down KubernetesAPIApprovalPolicyConformantConditionController
I1126 07:30:19.174004       1 crd_finalizer.go:281] Shutting down CRDFinalizer
I1126 07:30:19.174001       1 crdregistration_controller.go:145] Shutting down crd-autoregister controller
I1126 07:30:19.174008       1 nonstructuralschema_controller.go:207] Shutting down NonStructuralSchemaConditionController
I1126 07:30:19.174014       1 controller.go:170] Shutting down OpenAPI controller
I1126 07:30:19.174019       1 system_namespaces_controller.go:76] Shutting down system namespaces controller
I1126 07:30:19.174024       1 customresource_discovery_controller.go:332] Shutting down DiscoveryController
I1126 07:30:19.174143       1 storage_flowcontrol.go:172] APF bootstrap ensurer is exiting
I1126 07:30:19.174153       1 cluster_authentication_trust_controller.go:482] Shutting down cluster_authentication_trust_controller controller
I1126 07:30:19.174159       1 apf_controller.go:389] Shutting down API Priority and Fairness config worker
I1126 07:30:19.174162       1 local_available_controller.go:172] Shutting down LocalAvailability controller
I1126 07:30:19.174167       1 controller.go:120] Shutting down OpenAPI V3 controller
I1126 07:30:19.174176       1 controller.go:132] Ending legacy_token_tracking_controller
I1126 07:30:19.175107       1 controller.go:133] Shutting down legacy_token_tracking_controller
I1126 07:30:19.174212       1 gc_controller.go:91] Shutting down apiserver lease garbage collector
I1126 07:30:19.174219       1 remote_available_controller.go:441] Shutting down RemoteAvailability controller
I1126 07:30:19.174224       1 apiservice_controller.go:134] Shutting down APIServiceRegistrationController
I1126 07:30:19.174228       1 autoregister_controller.go:168] Shutting down autoregister controller
I1126 07:30:19.174388       1 controller.go:86] Shutting down OpenAPI V3 AggregationController
I1126 07:30:19.174418       1 controller.go:84] Shutting down OpenAPI AggregationController
I1126 07:30:19.175201       1 controller.go:176] quota evaluator worker shutdown
I1126 07:30:19.175211       1 controller.go:176] quota evaluator worker shutdown
I1126 07:30:19.175220       1 controller.go:176] quota evaluator worker shutdown
I1126 07:30:19.175216       1 controller.go:176] quota evaluator worker shutdown
I1126 07:30:19.175486       1 dynamic_cafile_content.go:175] "Shutting down controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 07:30:19.175534       1 repairip.go:246] Shutting down ipallocator-repair-controller
I1126 07:30:19.175597       1 dynamic_serving_content.go:149] "Shutting down controller" name="serving-cert::/etc/kubernetes/pki/apiserver.crt::/etc/kubernetes/pki/apiserver.key"
I1126 07:30:19.175609       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I1126 07:30:19.175653       1 secure_serving.go:259] Stopped listening on [::]:6443
I1126 07:30:19.175847       1 dynamic_cafile_content.go:175] "Shutting down controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$
