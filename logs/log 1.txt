master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ # 1. Backup
sudo cp /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml.$(date +%s).bak

# 2. เติม Authorization Mode (1.2.7, 1.2.8)
# (ปกติ Kubeadm มีให้อยู่แล้ว แต่ถ้า Fail แปลว่าอาจเขียนไม่ครบ)
sudo sed -i 's/--authorization-mode=Node,RBAC/--authorization-mode=Node,RBAC/g' /etc/kubernetes/manifests/kube-apiserver.yaml

# 3. เติม Service Account Config (1.2.22, 1.2.30)
sudo sed -i '/- kube-apiserver/a \    - --service-account-key-file=/etc/kubernetes/pki/sa.pub' /etc/kubernetes/manifests/kube-apiserver.yaml
# (ข้อ 1.2.30 Default ปลอดภัยอยู่แล้ว แต่ถ้าอยาก Explicit ก็เติมได้)

# 4. เติม Etcd Certs (1.2.23, 1.2.26) - สำคัญมาก!
sudo sed -i '/- kube-apiserver/a \    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt' /etc/kubernetes/manifests/kube-apiserver.yaml
sudo sed -i '/- kube-apiserver/a \    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key' /etc/kubernetes/manifests/kube-apiserver.yaml
sudo sed -i '/- kube-apiserver/a \    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt' /etc/kubernetes/manifests/kube-apiserver.yaml

# 5. เติม TLS (1.2.24)
sudo sed -i '/- kube-apiserver/a \    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt' /etc/kubernetes/manifests/kube-apiserver.yaml
sudo sed -i '/- kube-apiserver/a \    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key' /etc/kubernetes/manifests/kube-apiserver.yaml

# 6. เติม Kubelet Client Cert (1.2.4) - แก้ตัว
sudo sed -i '/- kube-apiserver/a \    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt' /etc/kubernetes/manifests/kube-apiserver.yaml
sudo sed -i '/- kube-apiserver/a \    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key' /etc/kubernetes/manifests/kube-apiserver.yaml
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ sudo systemctl restart kukelet
Failed to restart kukelet.service: Unit kukelet.service not found.
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ sudo systemctl restart kubelet
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ sudo python3 cis_k8s_master.py -vv

=== CIS Kubernetes Benchmark ===
Current Verbosity: Level 3
1. Run Audit (Full Scan)
2. Run Remediation (Full Fix)
3. Custom Scan (Select Level/Role)
4. Generate Reports Only
0. Exit
===========================================
Select option: 3
Level (1/2/all) [all]: 1
Role (master/worker/all) [all]: master
Mode (audit/remediate) [audit]: audit

=== Starting CIS AUDIT (Level: 1, Role: master) ===
[*] Found 92 scripts to execute.

✅ 1.1.15     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/scheduler.conf are 600

✅ 1.1.13     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/admin.conf are 600
      - Check Passed: Permissions on /etc/kubernetes/super-admin.conf are 600

❌ 1.1.12     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: Ownership on /var/lib/etcd is root:root (should be etcd:etcd)

✅ 1.1.10     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/cni/net.d/.kubernetes-cni-keep is root:root
      - Check Passed: Ownership on /etc/cni/net.d/10-flannel.conflist is root:root

✅ 1.1.18     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/controller-manager.conf is root:root

✅ 1.1.17     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/controller-manager.conf are 600

✅ 1.1.16     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/scheduler.conf is root:root

✅ 1.1.11     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /var/lib/etcd are 700

✅ 1.1.14     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/admin.conf is root:root
      - Check Passed: Ownership on /etc/kubernetes/super-admin.conf is root:root

✅ 1.1.1      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/manifests/kube-apiserver.yaml are 600

✅ 1.1.19     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: All files and directories in /etc/kubernetes/pki are owned by root:root

✅ 1.1.6      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/manifests/kube-scheduler.yaml is root:root

✅ 1.1.3      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/manifests/kube-controller-manager.yaml are 600

✅ 1.1.4      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/manifests/kube-controller-manager.yaml is root:root

✅ 1.1.8      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/manifests/etcd.yaml is root:root

✅ 1.1.2      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/manifests/kube-apiserver.yaml is root:root

✅ 1.1.5      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/manifests/kube-scheduler.yaml are 600

✅ 1.1.7      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/manifests/etcd.yaml are 600

✅ 1.1.9      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/cni/net.d/.kubernetes-cni-keep are 600
      - Check Passed: Permissions on /etc/cni/net.d/10-flannel.conflist are 600

❌ 1.2.15     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --profiling is not set to false

❌ 1.2.11     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: AlwaysPullImages is NOT enabled

✅ 1.1.21     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/pki/front-proxy-client.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/peer.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/server.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/ca.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/healthcheck-client.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/sa.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/ca.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver-etcd-client.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver-kubelet-client.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/front-proxy-ca.key are 600

❌ 1.2.17     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --audit-log-maxage is not set

❌ 1.2.18     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --audit-log-maxbackup is not set

❌ 1.2.1      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --anonymous-auth is NOT set to false

❌ 1.2.16     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --audit-log-path is not set

✅ 1.2.10     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: AlwaysAdmit is not enabled

✅ 1.1.20     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver-kubelet-client.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/peer.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/server.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/ca.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/healthcheck-client.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/front-proxy-ca.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/front-proxy-client.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver-etcd-client.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/ca.crt are 644

✅ 1.2.20     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --request-timeout is not set (using default)

❌ 1.2.19     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --audit-log-maxsize is not set

❌ 1.2.23     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --etcd-certfile and/or --etcd-keyfile are not set

✅ 1.2.21     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --service-account-lookup is true or not set (default true)

❌ 1.2.24     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --tls-cert-file and/or --tls-private-key-file are not set

❌ 1.2.22     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --service-account-key-file is not set

❌ 1.2.26     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --etcd-cafile is not set

❌ 1.2.27     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --encryption-provider-config is not set

❌ 1.2.25     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --client-ca-file is not set

❌ 1.2.28     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --encryption-provider-config is not set

❌ 1.2.29     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --tls-cipher-suites is not set

❌ 1.2.4      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --kubelet-client-certificate and/or --kubelet-client-key are NOT set

✅ 1.2.6      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --authorization-mode is not set to AlwaysAllow

❌ 1.2.30     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --service-account-extend-token-expiration is not set to false

✅ 1.2.2      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --token-auth-file is not set

❌ 1.2.8      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --authorization-mode does NOT include RBAC

❌ 1.2.3      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: DenyServiceExternalIPs is NOT enabled

✅ 1.3.2      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --profiling is set to false

❌ 1.2.5      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --kubelet-certificate-authority is NOT set

❌ 1.2.7      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --authorization-mode does NOT include Node

✅ 1.3.3      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --use-service-account-credentials is set to true

✅ 1.3.1      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --terminated-pod-gc-threshold is set

❌ 1.2.9      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: EventRateLimit is NOT enabled

✅ 1.3.5      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --root-ca-file is set

✅ 1.3.7      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --bind-address is set to 127.0.0.1

✅ 1.3.4      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --service-account-private-key-file is set

✅ 1.4.2      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --bind-address is set to 127.0.0.1

✅ 2.2        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --client-cert-auth is set to true

✅ 1.3.6      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: RotateKubeletServerCertificate is set to true

✅ 3.1.1      PASS     (Master)
   Output:

✅ 3.1.2      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Manual Check: --service-account-lookup is NOT set to true.

✅ 4.1.1      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf not found (Audit not applicable or file missing)

✅ 5.1.10     PASS     (Master)
   Output:

✅ 5.1.11     PASS     (Master)
   Output:

✅ 5.1.12     PASS     (Master)
   Output:

✅ 5.1.13     PASS     (Master)
   Output:

✅ 2.1        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --cert-file and --key-file are set

✅ 1.4.1      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --profiling is set to false

✅ 5.1.1      PASS     (Master)
   Output:

✅ 2.5        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --peer-client-cert-auth is set to true

✅ 3.1.3      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --enable-bootstrap-token-auth is not set to true

✅ 5.1.5      PASS     (Master)
   Output:

✅ 5.1.6      PASS     (Master)
   Output:

✅ 2.3        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --auto-tls is not set to true

✅ 5.1.7      PASS     (Master)
   Output:

✅ 2.6        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --peer-auto-tls is not set to true

❌ 3.2.1      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --audit-policy-file is not set

✅ 5.1.4      PASS     (Master)
   Output:

✅ 5.1.8      PASS     (Master)
   Output:

✅ 5.1.2      PASS     (Master)
   Output:

✅ 5.2.2      PASS     (Master)
   Output:

✅ 5.2.3      PASS     (Master)
   Output:

✅ 5.2.1      PASS     (Master)
   Output:

✅ 5.1.9      PASS     (Master)
   Output:

✅ 5.2.5      PASS     (Master)
   Output:

✅ 5.2.11     PASS     (Master)
   Output:

✅ 5.2.6      PASS     (Master)
   Output:

✅ 5.2.10     PASS     (Master)
   Output:

✅ 5.2.12     PASS     (Master)
   Output:

✅ 5.2.4      PASS     (Master)
   Output:

✅ 2.4        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --peer-cert-file and --peer-key-file are set

✅ 5.2.8      PASS     (Master)
   Output:

✅ 5.6.1      PASS     (Master)
   Output:

✅ 5.3.1      PASS     (Master)
   Output:


=== Execution Summary ===
Pass:  68
Fail:  24
Total: 92

[*] Report generated: /home/master/CIS_Kubernetes_Benchmark_V1.12.0/reports/cis_audit_report_20251126_030918.csv

------------------------------

1. Run Audit | 2. Run Fix | 3. Custom | 0. Exit
Select option: sudo nano /etc/kubernetes/manifests/kube-apiserver.yaml^[[D^[[D^[[D^[[D^[[D^[[D^[[D^[[D^[[D^[[D^[[D^[[D^Z
[1]+  Stopped                 sudo python3 cis_k8s_master.py -vv
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.150.131:6443
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-certificate-authority=/etc/kubernetes/pki/ca.crt
    - --anonymous-auth=false
    - --advertise-address=192.168.150.131
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction,AlwaysPullImages,DenyServiceExternalIPs,EventRateLimit
    - --enable-bootstrap-token-auth=false
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: registry.k8s.io/kube-apiserver:v1.34.2
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 192.168.150.131
        path: /livez
        port: probe-port
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    ports:
    - containerPort: 6443
      name: probe-port
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 192.168.150.131
        path: /readyz
        port: probe-port
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 192.168.150.131
        path: /livez
        port: probe-port
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ sudo python3 cis_k8s_master.py --audit

=== Starting CIS AUDIT (Level: all, Role: all) ===
[*] Found 131 scripts to execute.

✅ 1.1.11     PASS     (Master)
✅ 1.1.10     PASS     (Master)
❌ 1.1.12     FAIL     (Master) -> - Reason(s) for audit failure:...
✅ 1.1.14     PASS     (Master)
✅ 1.1.13     PASS     (Master)
✅ 1.1.18     PASS     (Master)
✅ 1.1.16     PASS     (Master)
✅ 1.1.15     PASS     (Master)
✅ 1.1.17     PASS     (Master)
✅ 1.1.19     PASS     (Master)
✅ 1.1.3      PASS     (Master)
✅ 1.1.4      PASS     (Master)
✅ 1.1.5      PASS     (Master)
✅ 1.1.8      PASS     (Master)
✅ 1.1.1      PASS     (Master)
✅ 1.1.6      PASS     (Master)
✅ 1.1.7      PASS     (Master)
✅ 1.1.2      PASS     (Master)
✅ 1.1.9      PASS     (Master)
✅ 1.1.20     PASS     (Master)
❌ 1.2.18     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.11     FAIL     (Master) -> - Reason(s) for audit failure:...
✅ 1.2.10     PASS     (Master)
❌ 1.2.19     FAIL     (Master) -> - Reason(s) for audit failure:...
✅ 1.1.21     PASS     (Master)
❌ 1.2.17     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.16     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.15     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.1      FAIL     (Master) -> - Reason(s) for audit failure:...
✅ 1.2.21     PASS     (Master)
✅ 1.2.20     PASS     (Master)
❌ 1.2.22     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.27     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.23     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.26     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.24     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.29     FAIL     (Master) -> - Reason(s) for audit failure:...
✅ 1.2.2      PASS     (Master)
❌ 1.2.25     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.30     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.28     FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.7      FAIL     (Master) -> - Reason(s) for audit failure:...
✅ 1.2.6      PASS     (Master)
❌ 1.2.3      FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.4      FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.5      FAIL     (Master) -> - Reason(s) for audit failure:...
✅ 1.3.1      PASS     (Master)
✅ 1.3.2      PASS     (Master)
❌ 1.2.8      FAIL     (Master) -> - Reason(s) for audit failure:...
❌ 1.2.9      FAIL     (Master) -> - Reason(s) for audit failure:...
✅ 1.3.4      PASS     (Master)
✅ 1.3.5      PASS     (Master)
✅ 1.3.3      PASS     (Master)
✅ 1.4.1      PASS     (Master)
✅ 1.3.6      PASS     (Master)
✅ 1.3.7      PASS     (Master)
✅ 1.4.2      PASS     (Master)
✅ 2.3        PASS     (Master)
✅ 2.2        PASS     (Master)
✅ 2.6        PASS     (Master)
✅ 2.5        PASS     (Master)
✅ 3.1.2      PASS     (Master)
✅ 5.1.10     PASS     (Master)
✅ 5.1.11     PASS     (Master)
✅ 4.1.1      PASS     (Master)
✅ 5.1.13     PASS     (Master)
✅ 5.1.12     PASS     (Master)
✅ 5.1.1      PASS     (Master)
✅ 5.1.7      PASS     (Master)
✅ 5.1.4      PASS     (Master)
✅ 5.1.2      PASS     (Master)
✅ 5.1.6      PASS     (Master)
✅ 3.1.3      PASS     (Master)
✅ 5.2.11     PASS     (Master)
✅ 5.2.10     PASS     (Master)
✅ 5.1.5      PASS     (Master)
✅ 5.1.9      PASS     (Master)
✅ 5.1.8      PASS     (Master)
✅ 3.1.1      PASS     (Master)
✅ 5.2.4      PASS     (Master)
✅ 5.2.12     PASS     (Master)
✅ 2.1        PASS     (Master)
✅ 5.2.3      PASS     (Master)
✅ 5.3.1      PASS     (Master)
✅ 2.4        PASS     (Master)
✅ 5.2.5      PASS     (Master)
❌ 3.2.1      FAIL     (Master) -> - Reason(s) for audit failure:...
✅ 5.2.6      PASS     (Master)
✅ 5.2.8      PASS     (Master)
✅ 5.6.1      PASS     (Master)
✅ 4.1.2      PASS     (Worker)
✅ 5.2.1      PASS     (Master)
✅ 5.2.2      PASS     (Master)
✅ 4.1.3      PASS     (Worker)
❌ 4.2.11     FAIL     (Worker) -> - Reason(s) for audit failure:...
✅ 4.1.5      PASS     (Worker)
✅ 4.1.10     PASS     (Worker)
✅ 4.1.8      PASS     (Worker)
✅ 4.1.6      PASS     (Worker)
✅ 4.2.10     PASS     (Worker)
✅ 4.1.4      PASS     (Worker)
✅ 4.1.7      PASS     (Worker)
❌ 4.2.12     FAIL     (Worker) -> - Reason(s) for audit failure:...
❌ 4.2.13     FAIL     (Worker) -> - Reason(s) for audit failure:...
✅ 4.1.9      PASS     (Worker)
❌ 4.2.14     FAIL     (Worker) -> - Reason(s) for audit failure:...
✅ 4.3.1      PASS     (Worker)
❌ 4.2.3      FAIL     (Worker) -> - Reason(s) for audit failure:...
❌ 4.2.1      FAIL     (Worker) -> - Reason(s) for audit failure:...
❌ 4.2.4      FAIL     (Worker) -> - Reason(s) for audit failure:...
✅ 5.1.3      PASS     (Worker)
✅ 2.7        PASS     (Master)
✅ 5.2.7      PASS     (Master)
✅ 4.2.5      PASS     (Worker)
✅ 3.2.2      PASS     (Master)
✅ 5.2.9      PASS     (Master)
✅ 4.2.7      PASS     (Worker)
✅ 5.4.2      PASS     (Master)
✅ 4.2.2      PASS     (Worker)
✅ 5.3.2      PASS     (Master)
✅ 5.6.2      PASS     (Master)
✅ 5.5.1      PASS     (Master)
✅ 5.4.1      PASS     (Master)
✅ 5.6.4      PASS     (Master)
✅ 1.2.13     PASS     (Master)
✅ 1.2.12     PASS     (Master)
✅ 5.6.3      PASS     (Master)
❌ 4.2.6      FAIL     (Worker) -> - Reason(s) for audit failure:...
❌ 4.2.9      FAIL     (Worker) -> - Reason(s) for audit failure:...
❌ 1.2.14     FAIL     (Master) -> - Reason(s) for audit failure:...
✅ 4.2.8      PASS     (Worker)

=== Execution Summary ===
Pass:  97
Fail:  34
Total: 131

[*] Report generated: /home/master/CIS_Kubernetes_Benchmark_V1.12.0/reports/cis_audit_report_20251126_031046.csv
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ sudo python3 cis_k8s_master.py --audit -vv

=== Starting CIS AUDIT (Level: all, Role: all) ===
[*] Found 131 scripts to execute.

✅ 1.1.14     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/admin.conf is root:root
      - Check Passed: Ownership on /etc/kubernetes/super-admin.conf is root:root

✅ 1.1.13     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/admin.conf are 600
      - Check Passed: Permissions on /etc/kubernetes/super-admin.conf are 600

❌ 1.1.12     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: Ownership on /var/lib/etcd is root:root (should be etcd:etcd)

✅ 1.1.16     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/scheduler.conf is root:root

✅ 1.1.15     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/scheduler.conf are 600

✅ 1.1.17     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/controller-manager.conf are 600

✅ 1.1.11     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /var/lib/etcd are 700

✅ 1.1.2      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/manifests/kube-apiserver.yaml is root:root

✅ 1.1.19     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: All files and directories in /etc/kubernetes/pki are owned by root:root

✅ 1.1.10     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/cni/net.d/.kubernetes-cni-keep is root:root
      - Check Passed: Ownership on /etc/cni/net.d/10-flannel.conflist is root:root

✅ 1.1.3      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/manifests/kube-controller-manager.yaml are 600

✅ 1.1.7      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/manifests/etcd.yaml are 600

✅ 1.1.1      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/manifests/kube-apiserver.yaml are 600

✅ 1.1.18     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/controller-manager.conf is root:root

✅ 1.1.8      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/manifests/etcd.yaml is root:root

✅ 1.1.5      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/manifests/kube-scheduler.yaml are 600

✅ 1.1.6      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/manifests/kube-scheduler.yaml is root:root

✅ 1.1.4      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Ownership on /etc/kubernetes/manifests/kube-controller-manager.yaml is root:root

✅ 1.1.9      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/cni/net.d/.kubernetes-cni-keep are 600
      - Check Passed: Permissions on /etc/cni/net.d/10-flannel.conflist are 600

✅ 1.2.10     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: AlwaysAdmit is not enabled

✅ 1.1.21     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/pki/front-proxy-client.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/peer.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/server.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/ca.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/healthcheck-client.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/sa.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/ca.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver-etcd-client.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver-kubelet-client.key are 600
      - Check Passed: Permissions on /etc/kubernetes/pki/front-proxy-ca.key are 600

❌ 1.2.15     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --profiling is not set to false

❌ 1.2.18     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --audit-log-maxbackup is not set

❌ 1.2.11     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: AlwaysPullImages is NOT enabled

❌ 1.2.17     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --audit-log-maxage is not set

✅ 1.1.20     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver-kubelet-client.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/peer.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/server.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/ca.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/etcd/healthcheck-client.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/front-proxy-ca.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/front-proxy-client.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/apiserver-etcd-client.crt are 644
      - Check Passed: Permissions on /etc/kubernetes/pki/ca.crt are 644

❌ 1.2.19     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --audit-log-maxsize is not set

❌ 1.2.1      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --anonymous-auth is NOT set to false

❌ 1.2.16     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --audit-log-path is not set

✅ 1.2.20     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --request-timeout is not set (using default)

❌ 1.2.25     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --client-ca-file is not set

❌ 1.2.27     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --encryption-provider-config is not set

❌ 1.2.22     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --service-account-key-file is not set

❌ 1.2.24     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --tls-cert-file and/or --tls-private-key-file are not set

❌ 1.2.29     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --tls-cipher-suites is not set

❌ 1.2.23     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --etcd-certfile and/or --etcd-keyfile are not set

❌ 1.2.28     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --encryption-provider-config is not set

✅ 1.2.21     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --service-account-lookup is true or not set (default true)

❌ 1.2.26     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --etcd-cafile is not set

✅ 1.2.2      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --token-auth-file is not set

❌ 1.2.5      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --kubelet-certificate-authority is NOT set

❌ 1.2.30     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --service-account-extend-token-expiration is not set to false

❌ 1.2.7      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --authorization-mode does NOT include Node

❌ 1.2.3      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: DenyServiceExternalIPs is NOT enabled

❌ 1.2.4      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --kubelet-client-certificate and/or --kubelet-client-key are NOT set

✅ 1.3.5      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --root-ca-file is set

✅ 1.3.3      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --use-service-account-credentials is set to true

✅ 1.3.4      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --service-account-private-key-file is set

✅ 1.3.6      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: RotateKubeletServerCertificate is set to true

✅ 1.2.6      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --authorization-mode is not set to AlwaysAllow

✅ 1.3.7      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --bind-address is set to 127.0.0.1

❌ 1.2.8      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --authorization-mode does NOT include RBAC

✅ 1.4.2      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --bind-address is set to 127.0.0.1

✅ 1.3.1      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --terminated-pod-gc-threshold is set

❌ 1.2.9      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: EventRateLimit is NOT enabled

✅ 1.3.2      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --profiling is set to false

✅ 2.3        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --auto-tls is not set to true

✅ 2.6        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --peer-auto-tls is not set to true

✅ 1.4.1      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --profiling is set to false

✅ 4.1.1      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf not found (Audit not applicable or file missing)

✅ 2.5        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --peer-client-cert-auth is set to true

✅ 5.1.10     PASS     (Master)
   Output:

✅ 5.1.11     PASS     (Master)
   Output:

✅ 2.2        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --client-cert-auth is set to true

✅ 5.1.1      PASS     (Master)
   Output:

✅ 5.1.12     PASS     (Master)
   Output:

✅ 5.1.13     PASS     (Master)
   Output:

✅ 3.1.2      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Manual Check: --service-account-lookup is NOT set to true.

✅ 3.1.1      PASS     (Master)
   Output:

✅ 5.1.4      PASS     (Master)
   Output:

✅ 5.1.8      PASS     (Master)
   Output:

✅ 5.1.6      PASS     (Master)
   Output:

✅ 5.1.9      PASS     (Master)
   Output:

✅ 5.2.11     PASS     (Master)
   Output:

✅ 5.1.5      PASS     (Master)
   Output:

✅ 5.2.10     PASS     (Master)
   Output:

✅ 3.1.3      PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --enable-bootstrap-token-auth is not set to true

❌ 3.2.1      FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --audit-policy-file is not set

✅ 5.2.1      PASS     (Master)
   Output:

✅ 2.1        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --cert-file and --key-file are set

✅ 5.1.7      PASS     (Master)
   Output:

✅ 2.4        PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --peer-cert-file and --peer-key-file are set

✅ 5.2.3      PASS     (Master)
   Output:

✅ 5.2.2      PASS     (Master)
   Output:

✅ 5.2.6      PASS     (Master)
   Output:

✅ 5.1.2      PASS     (Master)
   Output:

✅ 5.2.12     PASS     (Master)
   Output:

✅ 5.2.5      PASS     (Master)
   Output:

✅ 5.2.4      PASS     (Master)
   Output:

✅ 5.3.1      PASS     (Master)
   Output:

✅ 5.6.1      PASS     (Master)
   Output:

✅ 4.1.2      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf does not exist

✅ 5.2.8      PASS     (Master)
   Output:

✅ 4.1.10     PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: /var/lib/kubelet/config.yaml ownership is root:root

✅ 4.1.6      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: /etc/kubernetes/kubelet.conf ownership is root:root

✅ 4.1.5      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: /etc/kubernetes/kubelet.conf permissions are 600 or more restrictive

✅ 4.1.9      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: /var/lib/kubelet/config.yaml permissions are 600 or more restrictive

✅ 4.2.10     PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --rotate-certificates is NOT set to false

❌ 4.2.12     FAIL     (Worker) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: RotateKubeletServerCertificate is NOT enabled

✅ 4.1.3      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: kube-proxy not running or --kubeconfig not set

✅ 4.1.8      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --client-ca-file not set

❌ 4.2.13     FAIL     (Worker) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --pod-max-pids is NOT set

✅ 4.1.7      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --client-ca-file not set

✅ 4.1.4      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: kube-proxy not running or --kubeconfig not set

❌ 4.2.11     FAIL     (Worker) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --rotate-certificates is NOT set to true

✅ 4.3.1      PASS     (Worker)
   Output:

✅ 5.1.3      PASS     (Worker)
   Output:

❌ 4.2.14     FAIL     (Worker) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --seccomp-default is NOT set to true

✅ 4.2.2      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --authorization-mode is NOT set to AlwaysAllow

✅ 1.2.12     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --disable-admission-plugins is not set

❌ 4.2.3      FAIL     (Worker) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --client-ca-file is NOT set

❌ 4.2.9      FAIL     (Worker) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --tls-cert-file and/or --tls-private-key-file are NOT set

✅ 4.2.7      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --hostname-override is NOT set

❌ 4.2.1      FAIL     (Worker) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --anonymous-auth is NOT set to false

❌ 4.2.4      FAIL     (Worker) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --read-only-port is NOT set to 0

❌ 4.2.6      FAIL     (Worker) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: --make-iptables-util-chains is NOT set to true

✅ 5.3.2      PASS     (Master)
   Output:

✅ 2.7        PASS     (Master)
   Output:

✅ 3.2.2      PASS     (Master)
   Output:

✅ 5.2.7      PASS     (Master)
   Output:

✅ 5.2.9      PASS     (Master)
   Output:

✅ 5.6.3      PASS     (Master)
   Output:

✅ 5.4.1      PASS     (Master)
   Output:

✅ 5.5.1      PASS     (Master)
   Output:

✅ 4.2.5      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --streaming-connection-idle-timeout is NOT set to 0

✅ 5.4.2      PASS     (Master)
   Output:

✅ 5.6.2      PASS     (Master)
   Output:

❌ 1.2.14     FAIL     (Master) -> - Reason(s) for audit failure:...
   Output:

     - Audit Result:
       [-] FAIL
      - Reason(s) for audit failure:
      - Check Failed: NodeRestriction is NOT present in --enable-admission-plugins

✅ 5.6.4      PASS     (Master)
   Output:

✅ 1.2.13     PASS     (Master)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: --disable-admission-plugins is not set

✅ 4.2.8      PASS     (Worker)
   Output:

     - Audit Result:
       [+] PASS
      - Check Passed: eventRecordQPS not set (using default 5)


=== Execution Summary ===
Pass:  97
Fail:  34
Total: 131

[*] Report generated: /home/master/CIS_Kubernetes_Benchmark_V1.12.0/reports/cis_audit_report_20251126_031146.csv
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0$ cd kube-bench/
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl apply -f job.yaml
error: error validating "job.yaml": error validating data: failed to download openapi: Get "https://192.168.150.131:6443/openapi/v2?timeout=32s": dial tcp 192.168.150.131:6443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo kubectl apply -f job.yaml
error: error validating "job.yaml": error validating data: failed to download openapi: Get "http://localhost:8080/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo systemctl restart kubelet
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
Get "https://192.168.150.131:6443/api/v1/nodes?limit=500": dial tcp 192.168.150.131:6443: connect: connection refused - error from a previous attempt: read tcp 192.168.150.131:49562->192.168.150.131:6443: read: connection reset by peer
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo systemctl restart kubelet
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo systemctl restart kubelet# ดูว่า API Server ตายหรือไม่ (Exited)
sudo crictl ps -a | grep kube-apiserver
-bash: syntax error near unexpected token `('
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
59fdc0447202f       a5f569d49a979       Less than a second ago   Running             kube-apiserver            16                  b66eace3887d2       kube-apiserver-terramaster            kube-system
ca8c050941236       a5f569d49a979       11 seconds ago           Exited              kube-apiserver            15                  b66eace3887d2       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ # เอา Container ID จากคำสั่งข้างบนมาใส่
sudo crictl logs <CONTAINER_ID>^C
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ # เอา Container ID จากคำสั่งข้างบนมาใส่
sudo crictl logs ca8c050941236
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
E1126 03:17:20.341825   50600 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"ca8c050941236\": not found" containerID="ca8c050941236"
FATA[0000] rpc error: code = NotFound desc = an error occurred when try to find container "ca8c050941236": not found
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl logs 59fdc0447202f
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
E1126 03:17:26.945252   50680 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"59fdc0447202f\": not found" containerID="59fdc0447202f"
FATA[0000] rpc error: code = NotFound desc = an error occurred when try to find container "59fdc0447202f": not found
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo nano /etc/kubernetes/manifests/kube-apiserver.yaml
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo systemctl restart kubelet
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo systemctl restart kubeletsudo systemctl restart kubelet
Failed to restart kubeletsudo.service: Unit kubeletsudo.service not found.
Failed to restart systemctl.service: Unit systemctl.service not found.
Failed to restart restart.service: Unit restart.service not found.
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo systemctl restart kubeletsudo systemctl restart kubelet
Failed to restart kubeletsudo.service: Unit kubeletsudo.service not found.
Failed to restart systemctl.service: Unit systemctl.service not found.
Failed to restart restart.service: Unit restart.service not found.
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo systemctl restart kubelet
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
Get "https://192.168.150.131:6443/api/v1/nodes?limit=500": dial tcp 192.168.150.131:6443: connect: connection refused - error from a previous attempt: read tcp 192.168.150.131:38166->192.168.150.131:6443: read: connection reset by peer
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo nano /etc/kubernetes/manifests/kube-apiserver.yaml
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
0ed4acf290f1a       a5f569d49a979       15 seconds ago      Exited              kube-apiserver            24                  b66eace3887d2       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
0ed4acf290f1a       a5f569d49a979       17 seconds ago      Exited              kube-apiserver            24                  b66eace3887d2       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
0ed4acf290f1a       a5f569d49a979       19 seconds ago      Exited              kube-apiserver            24                  b66eace3887d2       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.150.131:6443
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-certificate-authority=/etc/kubernetes/pki/ca.crt
    - --anonymous-auth=false
    - --advertise-address=192.168.150.131
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction,AlwaysPullImages,DenyServiceExternalIPs
    - --enable-bootstrap-token-auth=false
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: registry.k8s.io/kube-apiserver:v1.34.2
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 192.168.150.131
        path: /livez
        port: probe-port
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    ports:
    - containerPort: 6443
      name: probe-port
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 192.168.150.131
        path: /readyz
        port: probe-port
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 192.168.150.131
        path: /livez
        port: probe-port
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
d6993aa41eefd       a5f569d49a979       About a minute ago   Exited              kube-apiserver            26                  b66eace3887d2       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl logs d6993aa41eefd
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
I1126 03:22:25.433016       1 options.go:263] external host was not specified, using 192.168.150.131
I1126 03:22:25.438035       1 server.go:150] Version: v1.34.2
I1126 03:22:25.438092       1 server.go:152] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
W1126 03:22:26.097060       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=internal.apiserver.k8s.io/v1alpha1
W1126 03:22:26.097136       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=authentication.k8s.io/v1alpha1
W1126 03:22:26.097149       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=resource.k8s.io/v1alpha3
W1126 03:22:26.097154       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=admissionregistration.k8s.io/v1alpha1
W1126 03:22:26.097159       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=certificates.k8s.io/v1alpha1
W1126 03:22:26.097163       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=rbac.authorization.k8s.io/v1alpha1
W1126 03:22:26.097166       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=scheduling.k8s.io/v1alpha1
W1126 03:22:26.097170       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storagemigration.k8s.io/v1alpha1
W1126 03:22:26.097173       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=coordination.k8s.io/v1alpha2
W1126 03:22:26.097177       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storage.k8s.io/v1alpha1
W1126 03:22:26.097181       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=imagepolicy.k8s.io/v1alpha1
W1126 03:22:26.097184       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=node.k8s.io/v1alpha1
W1126 03:22:26.124634       1 logging.go:55] [core] [Channel #1 SubChannel #2]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:22:26.125831       1 logging.go:55] [core] [Channel #4 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 03:22:26.133430       1 shared_informer.go:349] "Waiting for caches to sync" controller="node_authorizer"
E1126 03:22:26.135368       1 run.go:72] "command failed" err="failed to apply admission: couldn't init admission plugin \"EventRateLimit\": limits: Invalid value: null: must not be empty"
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.150.131:6443
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-certificate-authority=/etc/kubernetes/pki/ca.crt
    - --anonymous-auth=false
    - --advertise-address=192.168.150.131
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction,AlwaysPullImages,DenyServiceExternalIPs
    - --enable-bootstrap-token-auth=false
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: registry.k8s.io/kube-apiserver:v1.34.2
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 192.168.150.131
        path: /livez
        port: probe-port
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    ports:
    - containerPort: 6443
      name: probe-port
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 192.168.150.131
        path: /readyz
        port: probe-port
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 192.168.150.131
        path: /livez
        port: probe-port
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo nano /etc/kubernetes/manifests/kube-apiserver.yaml
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo systemctl restart kubelet
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
9522eecf7d1b8       a5f569d49a979       7 seconds ago       Exited              kube-apiserver            28                  b66eace3887d2       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl logs 9522eecf7d1b8
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
I1126 03:26:12.937825       1 options.go:263] external host was not specified, using 192.168.150.131
I1126 03:26:12.947982       1 server.go:150] Version: v1.34.2
I1126 03:26:12.948063       1 server.go:152] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
W1126 03:26:13.217801       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=admissionregistration.k8s.io/v1alpha1
W1126 03:26:13.217876       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=internal.apiserver.k8s.io/v1alpha1
W1126 03:26:13.217892       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=rbac.authorization.k8s.io/v1alpha1
W1126 03:26:13.217900       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=scheduling.k8s.io/v1alpha1
W1126 03:26:13.217906       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=certificates.k8s.io/v1alpha1
W1126 03:26:13.217912       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=imagepolicy.k8s.io/v1alpha1
W1126 03:26:13.217941       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=resource.k8s.io/v1alpha3
W1126 03:26:13.217949       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storage.k8s.io/v1alpha1
W1126 03:26:13.217954       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=authentication.k8s.io/v1alpha1
W1126 03:26:13.217959       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storagemigration.k8s.io/v1alpha1
W1126 03:26:13.217963       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=coordination.k8s.io/v1alpha2
W1126 03:26:13.217969       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=node.k8s.io/v1alpha1
W1126 03:26:13.261658       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:26:13.262819       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 03:26:13.263931       1 shared_informer.go:349] "Waiting for caches to sync" controller="node_authorizer"
E1126 03:26:13.267098       1 run.go:72] "command failed" err="failed to apply admission: couldn't init admission plugin \"EventRateLimit\": limits: Invalid value: null: must not be empty"
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo nano /etc/kubernetes/manifests/kube-apiserver.yaml
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ # 1. หยุด Kubelet ก่อน
sudo systemctl stop kubelet

# 2. ลบ Container ของ API Server ที่ตายซากอยู่ทิ้งให้หมด (สำคัญ!)
sudo crictl ps -a | grep kube-apiserver | awk '{print $1}' | xargs -r sudo crictl rm

# 3. เริ่ม Kubelet ใหม่
sudo systemctl start kubelet
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
b0205154c6baa
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
c163a3fc8e30d       a5f569d49a979       10 seconds ago      Exited              kube-apiserver            1                   b66eace3887d2       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl logs c163a3fc8e30d
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
I1126 03:30:08.142169       1 options.go:263] external host was not specified, using 192.168.150.131
I1126 03:30:08.144212       1 server.go:150] Version: v1.34.2
I1126 03:30:08.144252       1 server.go:152] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
W1126 03:30:08.666081       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=imagepolicy.k8s.io/v1alpha1
W1126 03:30:08.666122       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=internal.apiserver.k8s.io/v1alpha1
W1126 03:30:08.666131       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storagemigration.k8s.io/v1alpha1
W1126 03:30:08.666135       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=authentication.k8s.io/v1alpha1
W1126 03:30:08.666138       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=coordination.k8s.io/v1alpha2
W1126 03:30:08.666140       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=rbac.authorization.k8s.io/v1alpha1
W1126 03:30:08.666143       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=resource.k8s.io/v1alpha3
W1126 03:30:08.666146       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=scheduling.k8s.io/v1alpha1
W1126 03:30:08.666148       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storage.k8s.io/v1alpha1
W1126 03:30:08.666151       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=admissionregistration.k8s.io/v1alpha1
W1126 03:30:08.666154       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=certificates.k8s.io/v1alpha1
W1126 03:30:08.666157       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=node.k8s.io/v1alpha1
W1126 03:30:08.677180       1 logging.go:55] [core] [Channel #1 SubChannel #2]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:30:08.677828       1 logging.go:55] [core] [Channel #3 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 03:30:08.679916       1 shared_informer.go:349] "Waiting for caches to sync" controller="node_authorizer"
E1126 03:30:08.681334       1 run.go:72] "command failed" err="failed to apply admission: couldn't init admission plugin \"EventRateLimit\": limits: Invalid value: null: must not be empty"
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo systemctl stop kubelet
# รอ 5 วินาที
sleep 5
sudo systemctl start kubelet
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
d48638adc2a92       a5f569d49a979       8 seconds ago       Exited              kube-apiserver            5                   b66eace3887d2       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl logs d48638adc2a92
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
I1126 03:31:51.460410       1 options.go:263] external host was not specified, using 192.168.150.131
I1126 03:31:51.462168       1 server.go:150] Version: v1.34.2
I1126 03:31:51.462205       1 server.go:152] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
W1126 03:31:52.007034       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storagemigration.k8s.io/v1alpha1
W1126 03:31:52.007078       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storage.k8s.io/v1alpha1
W1126 03:31:52.007087       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=admissionregistration.k8s.io/v1alpha1
W1126 03:31:52.007091       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=certificates.k8s.io/v1alpha1
W1126 03:31:52.007094       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=scheduling.k8s.io/v1alpha1
W1126 03:31:52.007097       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=internal.apiserver.k8s.io/v1alpha1
W1126 03:31:52.007100       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=authentication.k8s.io/v1alpha1
W1126 03:31:52.007103       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=imagepolicy.k8s.io/v1alpha1
W1126 03:31:52.007105       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=coordination.k8s.io/v1alpha2
W1126 03:31:52.007108       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=rbac.authorization.k8s.io/v1alpha1
W1126 03:31:52.007111       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=resource.k8s.io/v1alpha3
W1126 03:31:52.007113       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=node.k8s.io/v1alpha1
W1126 03:31:52.022537       1 logging.go:55] [core] [Channel #1 SubChannel #2]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:31:52.022860       1 logging.go:55] [core] [Channel #4 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:31:52.025312       1 shared_informer.go:349] "Waiting for caches to sync" controller="node_authorizer"
E1126 03:31:52.027700       1 run.go:72] "command failed" err="failed to apply admission: couldn't init admission plugin \"EventRateLimit\": limits: Invalid value: null: must not be empty"
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ watch kubectl get nodes
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo mv /etc/kubernetes/manifests/kube-apiserver.yaml /home/master/kube-apiserver.yaml.temp
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ # รันวนไปเรื่อยๆ จนกว่าจะไม่เห็น kube-apiserver
watch sudo crictl ps -a
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ watch sudo crictl ps -a
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ cat /home/master/kube-apiserver.yaml.temp | grep admission-plugins
cat: /home/master/kube-apiserver.yaml.temp: Permission denied
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ suso cat /home/master/kube-apiserver.yaml.temp | grep admission-plugins
Command 'suso' not found, did you mean:
  command 'sumo' from deb sumo (1.12.0+dfsg1-1)
  command 'sudo' from deb sudo (1.9.9-1ubuntu2.5)
  command 'sudo' from deb sudo-ldap (1.9.9-1ubuntu2.5)
Try: sudo apt install <deb name>
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo cat /home/master/kube-apiserver.yaml.temp | grep admission-plugins
    - --enable-admission-plugins=NodeRestriction,AlwaysPullImages,DenyServiceExternalIPs
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ watch sudo crictl ps -a
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo mv /home/master/kube-apiserver.yaml.temp /etc/kubernetes/manifests/kube-apiserver.yaml
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo systemctl restart kubelet
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ watch kubectl get nodes
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ # สั่งลบ Container ทิ้งทันที (Kubelet จะตกใจแล้วสร้างตัวใหม่ให้เองใน 1 วินาที)
sudo crictl ps -a | grep kube-apiserver | awk '{print $1}' | xargs -r sudo crictl rm
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
6e699f2b7bde5
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo mv /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml.bak_trash
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo kubeadm init phase control-plane apiserver --apiserver-advertise-address 192.168.150.131 --pod-network-cidr=10.244.0.0/16
error: unknown flag: --pod-network-cidr
To see the stack trace of this error execute with --v=5 or higher
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo kubeadm init phase control-plane apiserver --apiserver-advertise-address 192.168.150.131
[control-plane] Creating static Pod manifest for "kube-apiserver"
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo systemctl restart kubelet
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
Error from server (Forbidden): nodes is forbidden: User "kubernetes-admin" cannot list resource "nodes" in API group "" at the cluster scope
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ watch kubectl get nodes
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
NAME          STATUS   ROLES           AGE   VERSION
terramaster   Ready    control-plane   21h   v1.34.2
terranode1    Ready    <none>          21h   v1.34.2
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl apply -f job.yaml
job.batch/kube-bench created
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get pods
NAME               READY   STATUS              RESTARTS   AGE
kube-bench-4lzdp   0/1     ContainerCreating   0          11s
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get pods
NAME               READY   STATUS              RESTARTS   AGE
kube-bench-4lzdp   0/1     ContainerCreating   0          20s
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl logs kube-bench-j76s9
error: error from server (NotFound): pods "kube-bench-j76s9" not found in namespace "default"
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get pods
NAME               READY   STATUS    RESTARTS   AGE
kube-bench-4lzdp   1/1     Running   0          41s
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ watch kubectl logs kube-bench-4lzdp
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl logs kube-bench-4lzdp
Error from server: Get "https://192.168.150.132:10250/containerLogs/default/kube-bench-4lzdp/kube-bench": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.150.132 because it doesn't contain any IP SANs
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get csr
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get pods
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ watch sudo crictl ps -a
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD                                   NAMESPACE
ccfe8c5f2d95e       a5f569d49a979       5 seconds ago       Running             kube-apiserver            2                   4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
8b60f3f92518a       01e8bacf0f500       9 seconds ago       Running             kube-controller-manager   2                   eed392ff39793       kube-controller-manager-terramaster   kube-system
600c1a1f0f73d       88320b5498ff2       14 seconds ago      Running             kube-scheduler            2                   352f9ed56f913       kube-scheduler-terramaster            kube-system
46a1226ee0744       a5f569d49a979       4 minutes ago       Exited              kube-apiserver            1                   4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
844fe7c0acaf5       01e8bacf0f500       4 minutes ago       Exited              kube-controller-manager   1                   eed392ff39793       kube-controller-manager-terramaster   kube-system
e5d6a25745bee       88320b5498ff2       4 minutes ago       Exited              kube-scheduler            1                   352f9ed56f913       kube-scheduler-terramaster            kube-system
09bf9ad41956d       52546a367cc9e       About an hour ago   Running             coredns                   1                   606b7d83d3033       coredns-66bc5c9577-jktg5              kube-system
c77746adef30f       52546a367cc9e       About an hour ago   Running             coredns                   1                   def7113ff7975       coredns-66bc5c9577-4pfwh              kube-system
25eea9f3c6072       e83704a177312       2 hours ago         Running             kube-flannel              1                   61fe1ce47c3a5       kube-flannel-ds-476m6                 kube-flannel
4e9b9f21c43d4       e83704a177312       2 hours ago         Exited              install-cni               0                   61fe1ce47c3a5       kube-flannel-ds-476m6                 kube-flannel
1aeb73921171a       bb28ded63816e       2 hours ago         Exited              install-cni-plugin        1                   61fe1ce47c3a5       kube-flannel-ds-476m6                 kube-flannel
28ae8cdbab273       8aa150647e88a       2 hours ago         Running             kube-proxy                1                   1c12f453f8db7       kube-proxy-xvsmn                      kube-system
6f3f218580753       a3e246e9556e9       2 hours ago         Running             etcd                      1                   c849ee45d6d3b       etcd-terramaster                      kube-system
5e9f42fcfd0c0       e83704a177312       21 hours ago        Exited              kube-flannel              0                   19c4ca4f5c116       kube-flannel-ds-476m6                 kube-flannel
53022640fd832       52546a367cc9e       22 hours ago        Exited              coredns                   0                   0308b7aab5cff       coredns-66bc5c9577-4pfwh              kube-system
c85887a9bbc10       52546a367cc9e       22 hours ago        Exited              coredns                   0                   a28ad1013ff3a       coredns-66bc5c9577-jktg5              kube-system
43752398026bb       8aa150647e88a       22 hours ago        Exited              kube-proxy                0                   d793f19f16541       kube-proxy-xvsmn                      kube-system
e0b0ee109090c       a3e246e9556e9       22 hours ago        Exited              etcd                      0                   62c40297b515e       etcd-terramaster                      kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
ccfe8c5f2d95e       a5f569d49a979       58 seconds ago       Running             kube-apiserver            2                   4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
46a1226ee0744       a5f569d49a979       5 minutes ago        Exited              kube-apiserver            1                   4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictส logs ccfe8c5f2d95e
sudo: crictส: command not found
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl logs ccfe8c5f2d95e
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
I1126 03:47:06.541651       1 options.go:263] external host was not specified, using 192.168.150.131
I1126 03:47:06.545034       1 server.go:150] Version: v1.34.2
I1126 03:47:06.545083       1 server.go:152] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
W1126 03:47:06.846503       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=certificates.k8s.io/v1alpha1
W1126 03:47:06.846562       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=imagepolicy.k8s.io/v1alpha1
W1126 03:47:06.846571       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=rbac.authorization.k8s.io/v1alpha1
W1126 03:47:06.846575       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=resource.k8s.io/v1alpha3
W1126 03:47:06.846578       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=authentication.k8s.io/v1alpha1
W1126 03:47:06.846580       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=scheduling.k8s.io/v1alpha1
W1126 03:47:06.846583       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=coordination.k8s.io/v1alpha2
W1126 03:47:06.846586       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storagemigration.k8s.io/v1alpha1
W1126 03:47:06.846589       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storage.k8s.io/v1alpha1
W1126 03:47:06.846592       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=admissionregistration.k8s.io/v1alpha1
W1126 03:47:06.846594       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=internal.apiserver.k8s.io/v1alpha1
W1126 03:47:06.846598       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=node.k8s.io/v1alpha1
W1126 03:47:06.860428       1 logging.go:55] [core] [Channel #1 SubChannel #2]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.862392       1 logging.go:55] [core] [Channel #4 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:47:06.863040       1 shared_informer.go:349] "Waiting for caches to sync" controller="node_authorizer"
I1126 03:47:06.873377       1 shared_informer.go:349] "Waiting for caches to sync" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1126 03:47:06.878109       1 plugins.go:157] Loaded 15 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,AlwaysPullImages,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,PodTopologyLabels,MutatingAdmissionPolicy,MutatingAdmissionWebhook.
I1126 03:47:06.878163       1 plugins.go:160] Loaded 15 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,AlwaysPullImages,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,DenyServiceExternalIPs,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I1126 03:47:06.878673       1 instance.go:239] Using reconciler: lease
W1126 03:47:06.880243       1 logging.go:55] [core] [Channel #9 SubChannel #10]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.887330       1 logging.go:55] [core] [Channel #13 SubChannel #14]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.896236       1 logging.go:55] [core] [Channel #21 SubChannel #22]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:47:06.908341       1 handler.go:285] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W1126 03:47:06.908392       1 genericapiserver.go:784] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I1126 03:47:06.911838       1 cidrallocator.go:197] starting ServiceCIDR Allocator Controller
W1126 03:47:06.912587       1 logging.go:55] [core] [Channel #27 SubChannel #28]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.921078       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.929365       1 logging.go:55] [core] [Channel #35 SubChannel #36]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.939176       1 logging.go:55] [core] [Channel #39 SubChannel #40]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.947916       1 logging.go:55] [core] [Channel #43 SubChannel #44]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.955281       1 logging.go:55] [core] [Channel #47 SubChannel #48]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.962921       1 logging.go:55] [core] [Channel #51 SubChannel #52]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.969461       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.980662       1 logging.go:55] [core] [Channel #59 SubChannel #60]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.987618       1 logging.go:55] [core] [Channel #63 SubChannel #64]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.994163       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.000877       1 logging.go:55] [core] [Channel #71 SubChannel #72]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.007785       1 logging.go:55] [core] [Channel #75 SubChannel #76]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.014933       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.022169       1 logging.go:55] [core] [Channel #83 SubChannel #84]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.029415       1 logging.go:55] [core] [Channel #87 SubChannel #88]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.036311       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 03:47:07.068855       1 handler.go:285] Adding GroupVersion  v1 to ResourceManager
I1126 03:47:07.069217       1 apis.go:112] API group "internal.apiserver.k8s.io" is not enabled, skipping.
W1126 03:47:07.071187       1 logging.go:55] [core] [Channel #95 SubChannel #96]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.079552       1 logging.go:55] [core] [Channel #99 SubChannel #100]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.088395       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.096272       1 logging.go:55] [core] [Channel #107 SubChannel #108]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.105370       1 logging.go:55] [core] [Channel #111 SubChannel #112]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.115137       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.125777       1 logging.go:55] [core] [Channel #119 SubChannel #120]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.135765       1 logging.go:55] [core] [Channel #123 SubChannel #124]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.147947       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.158120       1 logging.go:55] [core] [Channel #131 SubChannel #132]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.166706       1 logging.go:55] [core] [Channel #135 SubChannel #136]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.175651       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.184050       1 logging.go:55] [core] [Channel #143 SubChannel #144]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.192377       1 logging.go:55] [core] [Channel #147 SubChannel #148]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.200060       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.208148       1 logging.go:55] [core] [Channel #155 SubChannel #156]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.215235       1 logging.go:55] [core] [Channel #159 SubChannel #160]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.225135       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.232110       1 logging.go:55] [core] [Channel #167 SubChannel #168]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.240706       1 logging.go:55] [core] [Channel #171 SubChannel #172]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.250885       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.263428       1 logging.go:55] [core] [Channel #179 SubChannel #180]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.277018       1 logging.go:55] [core] [Channel #183 SubChannel #184]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.288561       1 logging.go:55] [core] [Channel #187 SubChannel #188]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:47:07.300778       1 apis.go:112] API group "storagemigration.k8s.io" is not enabled, skipping.
W1126 03:47:07.303798       1 logging.go:55] [core] [Channel #191 SubChannel #192]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.317230       1 logging.go:55] [core] [Channel #195 SubChannel #196]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.328591       1 logging.go:55] [core] [Channel #199 SubChannel #200]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.334278       1 logging.go:55] [core] [Channel #203 SubChannel #204]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.343498       1 logging.go:55] [core] [Channel #207 SubChannel #208]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.351860       1 logging.go:55] [core] [Channel #211 SubChannel #212]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.359000       1 logging.go:55] [core] [Channel #215 SubChannel #216]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.368068       1 logging.go:55] [core] [Channel #219 SubChannel #220]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.374234       1 logging.go:55] [core] [Channel #223 SubChannel #224]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.380879       1 logging.go:55] [core] [Channel #227 SubChannel #228]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.389657       1 logging.go:55] [core] [Channel #231 SubChannel #232]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.398801       1 logging.go:55] [core] [Channel #235 SubChannel #236]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.407024       1 logging.go:55] [core] [Channel #239 SubChannel #240]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.433392       1 logging.go:55] [core] [Channel #243 SubChannel #244]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.441091       1 logging.go:55] [core] [Channel #247 SubChannel #248]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.448504       1 logging.go:55] [core] [Channel #251 SubChannel #252]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:47:07.463587       1 handler.go:285] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W1126 03:47:07.463635       1 genericapiserver.go:784] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.463645       1 genericapiserver.go:784] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.463984       1 handler.go:285] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W1126 03:47:07.464016       1 genericapiserver.go:784] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I1126 03:47:07.464746       1 handler.go:285] Adding GroupVersion autoscaling v2 to ResourceManager
I1126 03:47:07.465406       1 handler.go:285] Adding GroupVersion autoscaling v1 to ResourceManager
W1126 03:47:07.465446       1 genericapiserver.go:784] Skipping API autoscaling/v2beta1 because it has no resources.
W1126 03:47:07.465454       1 genericapiserver.go:784] Skipping API autoscaling/v2beta2 because it has no resources.
I1126 03:47:07.466739       1 handler.go:285] Adding GroupVersion batch v1 to ResourceManager
W1126 03:47:07.466778       1 genericapiserver.go:784] Skipping API batch/v1beta1 because it has no resources.
I1126 03:47:07.467662       1 handler.go:285] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W1126 03:47:07.467709       1 genericapiserver.go:784] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.467720       1 genericapiserver.go:784] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.468354       1 handler.go:285] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W1126 03:47:07.468400       1 genericapiserver.go:784] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.468408       1 genericapiserver.go:784] Skipping API coordination.k8s.io/v1alpha2 because it has no resources.
I1126 03:47:07.469249       1 handler.go:285] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
W1126 03:47:07.469309       1 genericapiserver.go:784] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I1126 03:47:07.473031       1 handler.go:285] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W1126 03:47:07.473121       1 genericapiserver.go:784] Skipping API networking.k8s.io/v1beta1 because it has no resources.
I1126 03:47:07.473884       1 handler.go:285] Adding GroupVersion node.k8s.io v1 to ResourceManager
W1126 03:47:07.473943       1 genericapiserver.go:784] Skipping API node.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.473951       1 genericapiserver.go:784] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.475055       1 handler.go:285] Adding GroupVersion policy v1 to ResourceManager
W1126 03:47:07.475110       1 genericapiserver.go:784] Skipping API policy/v1beta1 because it has no resources.
I1126 03:47:07.477397       1 handler.go:285] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W1126 03:47:07.477437       1 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.477445       1 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.477909       1 handler.go:285] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W1126 03:47:07.477957       1 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.477963       1 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.480571       1 handler.go:285] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W1126 03:47:07.480613       1 genericapiserver.go:784] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.480621       1 genericapiserver.go:784] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.481911       1 handler.go:285] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
W1126 03:47:07.482117       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta3 because it has no resources.
W1126 03:47:07.482125       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
W1126 03:47:07.482128       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
I1126 03:47:07.486335       1 handler.go:285] Adding GroupVersion apps v1 to ResourceManager
W1126 03:47:07.486394       1 genericapiserver.go:784] Skipping API apps/v1beta2 because it has no resources.
W1126 03:47:07.486403       1 genericapiserver.go:784] Skipping API apps/v1beta1 because it has no resources.
I1126 03:47:07.489666       1 handler.go:285] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W1126 03:47:07.489770       1 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.489809       1 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.491042       1 handler.go:285] Adding GroupVersion events.k8s.io v1 to ResourceManager
W1126 03:47:07.491120       1 genericapiserver.go:784] Skipping API events.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.491236       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta2 because it has no resources.
I1126 03:47:07.498458       1 handler.go:285] Adding GroupVersion resource.k8s.io v1 to ResourceManager
W1126 03:47:07.498507       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.498518       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1alpha3 because it has no resources.
W1126 03:47:07.507965       1 logging.go:55] [core] [Channel #255 SubChannel #256]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 03:47:07.524727       1 handler.go:285] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1126 03:47:07.524800       1 genericapiserver.go:784] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1126 03:47:07.949782       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 03:47:07.949822       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1126 03:47:07.949999       1 secure_serving.go:211] Serving securely on [::]:6443
I1126 03:47:07.950001       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/etc/kubernetes/pki/apiserver.crt::/etc/kubernetes/pki/apiserver.key"
I1126 03:47:07.950187       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I1126 03:47:07.950268       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1126 03:47:07.950240       1 aggregator.go:169] waiting for initial CRD sync...
I1126 03:47:07.950561       1 controller.go:78] Starting OpenAPI AggregationController
I1126 03:47:07.950740       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1126 03:47:07.951009       1 local_available_controller.go:156] Starting LocalAvailability controller
I1126 03:47:07.951261       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1126 03:47:07.951439       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1126 03:47:07.951670       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1126 03:47:07.951959       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1126 03:47:07.952077       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
I1126 03:47:07.952267       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1126 03:47:07.952484       1 repairip.go:210] Starting ipallocator-repair-controller
I1126 03:47:07.952654       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I1126 03:47:07.950019       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1126 03:47:07.953270       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I1126 03:47:07.954528       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1126 03:47:07.954401       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/etc/kubernetes/pki/front-proxy-client.crt::/etc/kubernetes/pki/front-proxy-client.key"
I1126 03:47:07.953287       1 controller.go:119] Starting legacy_token_tracking_controller
I1126 03:47:07.955354       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I1126 03:47:07.954172       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1126 03:47:07.954361       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I1126 03:47:07.950225       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1126 03:47:07.978049       1 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
I1126 03:47:07.978370       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I1126 03:47:07.978558       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1126 03:47:07.978996       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 03:47:07.979425       1 controller.go:142] Starting OpenAPI controller
I1126 03:47:07.979449       1 controller.go:90] Starting OpenAPI V3 controller
I1126 03:47:07.979456       1 naming_controller.go:299] Starting NamingConditionController
I1126 03:47:07.979463       1 establishing_controller.go:81] Starting EstablishingController
I1126 03:47:07.979471       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1126 03:47:07.979476       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1126 03:47:07.979484       1 crd_finalizer.go:269] Starting CRDFinalizer
I1126 03:47:08.075393       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1126 03:47:08.075500       1 policy_source.go:240] refreshing policies
I1126 03:47:08.078611       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1126 03:47:08.078758       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1126 03:47:08.081717       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1126 03:47:08.083340       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1126 03:47:08.112906       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1126 03:47:08.151616       1 cache.go:39] Caches are synced for LocalAvailability controller
I1126 03:47:08.151683       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1126 03:47:08.151694       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1126 03:47:08.151999       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1126 03:47:08.152998       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1126 03:47:08.153067       1 aggregator.go:171] initial CRD sync complete...
I1126 03:47:08.153075       1 autoregister_controller.go:144] Starting autoregister controller
I1126 03:47:08.153079       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1126 03:47:08.153082       1 cache.go:39] Caches are synced for autoregister controller
I1126 03:47:08.154117       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1126 03:47:08.155129       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1126 03:47:08.155227       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1126 03:47:08.156107       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1126 03:47:08.163685       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1126 03:47:08.957347       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W1126 03:47:09.162484       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.150.131]
I1126 03:47:09.163609       1 controller.go:667] quota admission added evaluator for: endpoints
I1126 03:47:09.168745       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1126 03:47:27.487617       1 controller.go:667] quota admission added evaluator for: serviceaccounts
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get pods
NAME               READY   STATUS    RESTARTS   AGE
kube-bench-4lzdp   1/1     Running   0          6m33s
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get node
NAME          STATUS   ROLES           AGE   VERSION
terramaster   Ready    control-plane   21h   v1.34.2
terranode1    Ready    <none>          21h   v1.34.2
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get csr
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get node
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
9a12c9526f7fa       a5f569d49a979       About a minute ago   Running             kube-apiserver            3                   4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
ccfe8c5f2d95e       a5f569d49a979       5 minutes ago        Exited              kube-apiserver            2                   4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo grep "enable-admission-plugins" /etc/kubernetes/manifests/kube-apiserver.yaml
    - --enable-admission-plugins=NodeRestriction
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl logs ccfe8c5f2d95e
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
I1126 03:47:06.541651       1 options.go:263] external host was not specified, using 192.168.150.131
I1126 03:47:06.545034       1 server.go:150] Version: v1.34.2
I1126 03:47:06.545083       1 server.go:152] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
W1126 03:47:06.846503       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=certificates.k8s.io/v1alpha1
W1126 03:47:06.846562       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=imagepolicy.k8s.io/v1alpha1
W1126 03:47:06.846571       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=rbac.authorization.k8s.io/v1alpha1
W1126 03:47:06.846575       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=resource.k8s.io/v1alpha3
W1126 03:47:06.846578       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=authentication.k8s.io/v1alpha1
W1126 03:47:06.846580       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=scheduling.k8s.io/v1alpha1
W1126 03:47:06.846583       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=coordination.k8s.io/v1alpha2
W1126 03:47:06.846586       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storagemigration.k8s.io/v1alpha1
W1126 03:47:06.846589       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storage.k8s.io/v1alpha1
W1126 03:47:06.846592       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=admissionregistration.k8s.io/v1alpha1
W1126 03:47:06.846594       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=internal.apiserver.k8s.io/v1alpha1
W1126 03:47:06.846598       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=node.k8s.io/v1alpha1
W1126 03:47:06.860428       1 logging.go:55] [core] [Channel #1 SubChannel #2]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.862392       1 logging.go:55] [core] [Channel #4 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:47:06.863040       1 shared_informer.go:349] "Waiting for caches to sync" controller="node_authorizer"
I1126 03:47:06.873377       1 shared_informer.go:349] "Waiting for caches to sync" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1126 03:47:06.878109       1 plugins.go:157] Loaded 15 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,AlwaysPullImages,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,PodTopologyLabels,MutatingAdmissionPolicy,MutatingAdmissionWebhook.
I1126 03:47:06.878163       1 plugins.go:160] Loaded 15 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,AlwaysPullImages,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,DenyServiceExternalIPs,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I1126 03:47:06.878673       1 instance.go:239] Using reconciler: lease
W1126 03:47:06.880243       1 logging.go:55] [core] [Channel #9 SubChannel #10]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.887330       1 logging.go:55] [core] [Channel #13 SubChannel #14]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.896236       1 logging.go:55] [core] [Channel #21 SubChannel #22]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:47:06.908341       1 handler.go:285] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W1126 03:47:06.908392       1 genericapiserver.go:784] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I1126 03:47:06.911838       1 cidrallocator.go:197] starting ServiceCIDR Allocator Controller
W1126 03:47:06.912587       1 logging.go:55] [core] [Channel #27 SubChannel #28]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.921078       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.929365       1 logging.go:55] [core] [Channel #35 SubChannel #36]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.939176       1 logging.go:55] [core] [Channel #39 SubChannel #40]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.947916       1 logging.go:55] [core] [Channel #43 SubChannel #44]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.955281       1 logging.go:55] [core] [Channel #47 SubChannel #48]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.962921       1 logging.go:55] [core] [Channel #51 SubChannel #52]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.969461       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.980662       1 logging.go:55] [core] [Channel #59 SubChannel #60]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.987618       1 logging.go:55] [core] [Channel #63 SubChannel #64]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:06.994163       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.000877       1 logging.go:55] [core] [Channel #71 SubChannel #72]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.007785       1 logging.go:55] [core] [Channel #75 SubChannel #76]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.014933       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.022169       1 logging.go:55] [core] [Channel #83 SubChannel #84]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.029415       1 logging.go:55] [core] [Channel #87 SubChannel #88]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.036311       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 03:47:07.068855       1 handler.go:285] Adding GroupVersion  v1 to ResourceManager
I1126 03:47:07.069217       1 apis.go:112] API group "internal.apiserver.k8s.io" is not enabled, skipping.
W1126 03:47:07.071187       1 logging.go:55] [core] [Channel #95 SubChannel #96]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.079552       1 logging.go:55] [core] [Channel #99 SubChannel #100]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.088395       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.096272       1 logging.go:55] [core] [Channel #107 SubChannel #108]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.105370       1 logging.go:55] [core] [Channel #111 SubChannel #112]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.115137       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.125777       1 logging.go:55] [core] [Channel #119 SubChannel #120]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.135765       1 logging.go:55] [core] [Channel #123 SubChannel #124]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.147947       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.158120       1 logging.go:55] [core] [Channel #131 SubChannel #132]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.166706       1 logging.go:55] [core] [Channel #135 SubChannel #136]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.175651       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.184050       1 logging.go:55] [core] [Channel #143 SubChannel #144]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.192377       1 logging.go:55] [core] [Channel #147 SubChannel #148]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.200060       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.208148       1 logging.go:55] [core] [Channel #155 SubChannel #156]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.215235       1 logging.go:55] [core] [Channel #159 SubChannel #160]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.225135       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.232110       1 logging.go:55] [core] [Channel #167 SubChannel #168]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.240706       1 logging.go:55] [core] [Channel #171 SubChannel #172]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.250885       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.263428       1 logging.go:55] [core] [Channel #179 SubChannel #180]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.277018       1 logging.go:55] [core] [Channel #183 SubChannel #184]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.288561       1 logging.go:55] [core] [Channel #187 SubChannel #188]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:47:07.300778       1 apis.go:112] API group "storagemigration.k8s.io" is not enabled, skipping.
W1126 03:47:07.303798       1 logging.go:55] [core] [Channel #191 SubChannel #192]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.317230       1 logging.go:55] [core] [Channel #195 SubChannel #196]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.328591       1 logging.go:55] [core] [Channel #199 SubChannel #200]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.334278       1 logging.go:55] [core] [Channel #203 SubChannel #204]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.343498       1 logging.go:55] [core] [Channel #207 SubChannel #208]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.351860       1 logging.go:55] [core] [Channel #211 SubChannel #212]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.359000       1 logging.go:55] [core] [Channel #215 SubChannel #216]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.368068       1 logging.go:55] [core] [Channel #219 SubChannel #220]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.374234       1 logging.go:55] [core] [Channel #223 SubChannel #224]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.380879       1 logging.go:55] [core] [Channel #227 SubChannel #228]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:47:07.389657       1 logging.go:55] [core] [Channel #231 SubChannel #232]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.398801       1 logging.go:55] [core] [Channel #235 SubChannel #236]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.407024       1 logging.go:55] [core] [Channel #239 SubChannel #240]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.433392       1 logging.go:55] [core] [Channel #243 SubChannel #244]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.441091       1 logging.go:55] [core] [Channel #247 SubChannel #248]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:47:07.448504       1 logging.go:55] [core] [Channel #251 SubChannel #252]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:47:07.463587       1 handler.go:285] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W1126 03:47:07.463635       1 genericapiserver.go:784] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.463645       1 genericapiserver.go:784] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.463984       1 handler.go:285] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W1126 03:47:07.464016       1 genericapiserver.go:784] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I1126 03:47:07.464746       1 handler.go:285] Adding GroupVersion autoscaling v2 to ResourceManager
I1126 03:47:07.465406       1 handler.go:285] Adding GroupVersion autoscaling v1 to ResourceManager
W1126 03:47:07.465446       1 genericapiserver.go:784] Skipping API autoscaling/v2beta1 because it has no resources.
W1126 03:47:07.465454       1 genericapiserver.go:784] Skipping API autoscaling/v2beta2 because it has no resources.
I1126 03:47:07.466739       1 handler.go:285] Adding GroupVersion batch v1 to ResourceManager
W1126 03:47:07.466778       1 genericapiserver.go:784] Skipping API batch/v1beta1 because it has no resources.
I1126 03:47:07.467662       1 handler.go:285] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W1126 03:47:07.467709       1 genericapiserver.go:784] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.467720       1 genericapiserver.go:784] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.468354       1 handler.go:285] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W1126 03:47:07.468400       1 genericapiserver.go:784] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.468408       1 genericapiserver.go:784] Skipping API coordination.k8s.io/v1alpha2 because it has no resources.
I1126 03:47:07.469249       1 handler.go:285] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
W1126 03:47:07.469309       1 genericapiserver.go:784] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I1126 03:47:07.473031       1 handler.go:285] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W1126 03:47:07.473121       1 genericapiserver.go:784] Skipping API networking.k8s.io/v1beta1 because it has no resources.
I1126 03:47:07.473884       1 handler.go:285] Adding GroupVersion node.k8s.io v1 to ResourceManager
W1126 03:47:07.473943       1 genericapiserver.go:784] Skipping API node.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.473951       1 genericapiserver.go:784] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.475055       1 handler.go:285] Adding GroupVersion policy v1 to ResourceManager
W1126 03:47:07.475110       1 genericapiserver.go:784] Skipping API policy/v1beta1 because it has no resources.
I1126 03:47:07.477397       1 handler.go:285] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W1126 03:47:07.477437       1 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.477445       1 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.477909       1 handler.go:285] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W1126 03:47:07.477957       1 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.477963       1 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.480571       1 handler.go:285] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W1126 03:47:07.480613       1 genericapiserver.go:784] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.480621       1 genericapiserver.go:784] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.481911       1 handler.go:285] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
W1126 03:47:07.482117       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta3 because it has no resources.
W1126 03:47:07.482125       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
W1126 03:47:07.482128       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
I1126 03:47:07.486335       1 handler.go:285] Adding GroupVersion apps v1 to ResourceManager
W1126 03:47:07.486394       1 genericapiserver.go:784] Skipping API apps/v1beta2 because it has no resources.
W1126 03:47:07.486403       1 genericapiserver.go:784] Skipping API apps/v1beta1 because it has no resources.
I1126 03:47:07.489666       1 handler.go:285] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W1126 03:47:07.489770       1 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.489809       1 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I1126 03:47:07.491042       1 handler.go:285] Adding GroupVersion events.k8s.io v1 to ResourceManager
W1126 03:47:07.491120       1 genericapiserver.go:784] Skipping API events.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.491236       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta2 because it has no resources.
I1126 03:47:07.498458       1 handler.go:285] Adding GroupVersion resource.k8s.io v1 to ResourceManager
W1126 03:47:07.498507       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta1 because it has no resources.
W1126 03:47:07.498518       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1alpha3 because it has no resources.
W1126 03:47:07.507965       1 logging.go:55] [core] [Channel #255 SubChannel #256]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 03:47:07.524727       1 handler.go:285] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1126 03:47:07.524800       1 genericapiserver.go:784] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1126 03:47:07.949782       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 03:47:07.949822       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1126 03:47:07.949999       1 secure_serving.go:211] Serving securely on [::]:6443
I1126 03:47:07.950001       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/etc/kubernetes/pki/apiserver.crt::/etc/kubernetes/pki/apiserver.key"
I1126 03:47:07.950187       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I1126 03:47:07.950268       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1126 03:47:07.950240       1 aggregator.go:169] waiting for initial CRD sync...
I1126 03:47:07.950561       1 controller.go:78] Starting OpenAPI AggregationController
I1126 03:47:07.950740       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1126 03:47:07.951009       1 local_available_controller.go:156] Starting LocalAvailability controller
I1126 03:47:07.951261       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1126 03:47:07.951439       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1126 03:47:07.951670       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1126 03:47:07.951959       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1126 03:47:07.952077       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
I1126 03:47:07.952267       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1126 03:47:07.952484       1 repairip.go:210] Starting ipallocator-repair-controller
I1126 03:47:07.952654       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I1126 03:47:07.950019       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1126 03:47:07.953270       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I1126 03:47:07.954528       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1126 03:47:07.954401       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/etc/kubernetes/pki/front-proxy-client.crt::/etc/kubernetes/pki/front-proxy-client.key"
I1126 03:47:07.953287       1 controller.go:119] Starting legacy_token_tracking_controller
I1126 03:47:07.955354       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I1126 03:47:07.954172       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1126 03:47:07.954361       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I1126 03:47:07.950225       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1126 03:47:07.978049       1 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
I1126 03:47:07.978370       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I1126 03:47:07.978558       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1126 03:47:07.978996       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 03:47:07.979425       1 controller.go:142] Starting OpenAPI controller
I1126 03:47:07.979449       1 controller.go:90] Starting OpenAPI V3 controller
I1126 03:47:07.979456       1 naming_controller.go:299] Starting NamingConditionController
I1126 03:47:07.979463       1 establishing_controller.go:81] Starting EstablishingController
I1126 03:47:07.979471       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1126 03:47:07.979476       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1126 03:47:07.979484       1 crd_finalizer.go:269] Starting CRDFinalizer
I1126 03:47:08.075393       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1126 03:47:08.075500       1 policy_source.go:240] refreshing policies
I1126 03:47:08.078611       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1126 03:47:08.078758       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1126 03:47:08.081717       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1126 03:47:08.083340       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1126 03:47:08.112906       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1126 03:47:08.151616       1 cache.go:39] Caches are synced for LocalAvailability controller
I1126 03:47:08.151683       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1126 03:47:08.151694       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1126 03:47:08.151999       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1126 03:47:08.152998       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1126 03:47:08.153067       1 aggregator.go:171] initial CRD sync complete...
I1126 03:47:08.153075       1 autoregister_controller.go:144] Starting autoregister controller
I1126 03:47:08.153079       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1126 03:47:08.153082       1 cache.go:39] Caches are synced for autoregister controller
I1126 03:47:08.154117       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1126 03:47:08.155129       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1126 03:47:08.155227       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1126 03:47:08.156107       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1126 03:47:08.163685       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1126 03:47:08.957347       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W1126 03:47:09.162484       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.150.131]
I1126 03:47:09.163609       1 controller.go:667] quota admission added evaluator for: endpoints
I1126 03:47:09.168745       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1126 03:47:27.487617       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1126 03:51:06.206107       1 controller.go:128] Shutting down kubernetes service endpoint reconciler
W1126 03:51:06.210495       1 lease.go:265] Resetting endpoints for master service "kubernetes" to []
I1126 03:51:06.218140       1 dynamic_cafile_content.go:175] "Shutting down controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 03:51:06.218561       1 storage_flowcontrol.go:172] APF bootstrap ensurer is exiting
I1126 03:51:06.218597       1 cluster_authentication_trust_controller.go:482] Shutting down cluster_authentication_trust_controller controller
I1126 03:51:06.218819       1 controller.go:132] Ending legacy_token_tracking_controller
I1126 03:51:06.218842       1 controller.go:133] Shutting down legacy_token_tracking_controller
I1126 03:51:06.218857       1 apiservice_controller.go:134] Shutting down APIServiceRegistrationController
I1126 03:51:06.218868       1 autoregister_controller.go:168] Shutting down autoregister controller
I1126 03:51:06.218884       1 crdregistration_controller.go:145] Shutting down crd-autoregister controller
I1126 03:51:06.218902       1 system_namespaces_controller.go:76] Shutting down system namespaces controller
I1126 03:51:06.218915       1 apf_controller.go:389] Shutting down API Priority and Fairness config worker
I1126 03:51:06.218934       1 local_available_controller.go:172] Shutting down LocalAvailability controller
I1126 03:51:06.218949       1 nonstructuralschema_controller.go:207] Shutting down NonStructuralSchemaConditionController
I1126 03:51:06.218960       1 establishing_controller.go:92] Shutting down EstablishingController
I1126 03:51:06.218971       1 controller.go:120] Shutting down OpenAPI V3 controller
I1126 03:51:06.218984       1 apiapproval_controller.go:201] Shutting down KubernetesAPIApprovalPolicyConformantConditionController
I1126 03:51:06.218993       1 naming_controller.go:310] Shutting down NamingConditionController
I1126 03:51:06.219000       1 crd_finalizer.go:281] Shutting down CRDFinalizer
I1126 03:51:06.219011       1 controller.go:170] Shutting down OpenAPI controller
I1126 03:51:06.219043       1 remote_available_controller.go:441] Shutting down RemoteAvailability controller
I1126 03:51:06.219060       1 customresource_discovery_controller.go:332] Shutting down DiscoveryController
I1126 03:51:06.219076       1 gc_controller.go:91] Shutting down apiserver lease garbage collector
I1126 03:51:06.219576       1 dynamic_cafile_content.go:175] "Shutting down controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1126 03:51:06.219583       1 dynamic_cafile_content.go:175] "Shutting down controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 03:51:06.219605       1 controller.go:86] Shutting down OpenAPI V3 AggregationController
I1126 03:51:06.219685       1 dynamic_serving_content.go:149] "Shutting down controller" name="aggregator-proxy-cert::/etc/kubernetes/pki/front-proxy-client.crt::/etc/kubernetes/pki/front-proxy-client.key"
I1126 03:51:06.219722       1 object_count_tracker.go:141] "StorageObjectCountTracker pruner is exiting"
I1126 03:51:06.219739       1 controller.go:84] Shutting down OpenAPI AggregationController
I1126 03:51:06.219750       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I1126 03:51:06.219806       1 secure_serving.go:259] Stopped listening on [::]:6443
I1126 03:51:06.219855       1 repairip.go:246] Shutting down ipallocator-repair-controller
I1126 03:51:06.220006       1 dynamic_serving_content.go:149] "Shutting down controller" name="serving-cert::/etc/kubernetes/pki/apiserver.crt::/etc/kubernetes/pki/apiserver.key"
I1126 03:51:06.220669       1 controller.go:157] Shutting down quota evaluator
I1126 03:51:06.220680       1 controller.go:176] quota evaluator worker shutdown
I1126 03:51:06.220670       1 dynamic_cafile_content.go:175] "Shutting down controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1126 03:51:06.220692       1 controller.go:176] quota evaluator worker shutdown
I1126 03:51:06.220697       1 controller.go:176] quota evaluator worker shutdown
I1126 03:51:06.220700       1 controller.go:176] quota evaluator worker shutdown
I1126 03:51:06.220704       1 controller.go:176] quota evaluator worker shutdown
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl logs 9a12c9526f7fa
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
I1126 03:51:36.521523       1 options.go:263] external host was not specified, using 192.168.150.131
I1126 03:51:36.526247       1 server.go:150] Version: v1.34.2
I1126 03:51:36.526413       1 server.go:152] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
W1126 03:51:36.844549       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=scheduling.k8s.io/v1alpha1
W1126 03:51:36.844626       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storage.k8s.io/v1alpha1
W1126 03:51:36.844645       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=internal.apiserver.k8s.io/v1alpha1
W1126 03:51:36.844649       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=certificates.k8s.io/v1alpha1
W1126 03:51:36.844652       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=imagepolicy.k8s.io/v1alpha1
W1126 03:51:36.844655       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=admissionregistration.k8s.io/v1alpha1
W1126 03:51:36.844658       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=coordination.k8s.io/v1alpha2
W1126 03:51:36.844661       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=resource.k8s.io/v1alpha3
W1126 03:51:36.844666       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=authentication.k8s.io/v1alpha1
W1126 03:51:36.844671       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=storagemigration.k8s.io/v1alpha1
W1126 03:51:36.844678       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=rbac.authorization.k8s.io/v1alpha1
W1126 03:51:36.844682       1 api_enablement.go:112] alpha api enabled with emulated version 1.34 instead of the binary's version 1.34.2, this is unsupported, proceed at your own risk: api=node.k8s.io/v1alpha1
W1126 03:51:36.866392       1 logging.go:55] [core] [Channel #4 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:36.867637       1 logging.go:55] [core] [Channel #1 SubChannel #2]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 03:51:36.868228       1 shared_informer.go:349] "Waiting for caches to sync" controller="node_authorizer"
I1126 03:51:36.882530       1 shared_informer.go:349] "Waiting for caches to sync" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1126 03:51:36.893057       1 plugins.go:157] Loaded 15 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,AlwaysPullImages,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,PodTopologyLabels,MutatingAdmissionPolicy,MutatingAdmissionWebhook.
I1126 03:51:36.893117       1 plugins.go:160] Loaded 15 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,AlwaysPullImages,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,DenyServiceExternalIPs,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I1126 03:51:36.893770       1 instance.go:239] Using reconciler: lease
W1126 03:51:36.895528       1 logging.go:55] [core] [Channel #9 SubChannel #10]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:36.904765       1 logging.go:55] [core] [Channel #13 SubChannel #14]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:36.917787       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:51:36.928212       1 handler.go:285] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W1126 03:51:36.928264       1 genericapiserver.go:784] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I1126 03:51:36.931914       1 cidrallocator.go:197] starting ServiceCIDR Allocator Controller
W1126 03:51:36.932728       1 logging.go:55] [core] [Channel #27 SubChannel #28]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:36.942486       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:36.950592       1 logging.go:55] [core] [Channel #35 SubChannel #36]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:36.960675       1 logging.go:55] [core] [Channel #39 SubChannel #40]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:36.968584       1 logging.go:55] [core] [Channel #43 SubChannel #44]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:36.978485       1 logging.go:55] [core] [Channel #47 SubChannel #48]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:36.986077       1 logging.go:55] [core] [Channel #51 SubChannel #52]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:36.997083       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.006112       1 logging.go:55] [core] [Channel #59 SubChannel #60]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.013542       1 logging.go:55] [core] [Channel #63 SubChannel #64]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.022996       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.032718       1 logging.go:55] [core] [Channel #71 SubChannel #72]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.040472       1 logging.go:55] [core] [Channel #75 SubChannel #76]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.050726       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.062736       1 logging.go:55] [core] [Channel #83 SubChannel #84]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.070409       1 logging.go:55] [core] [Channel #87 SubChannel #88]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.076908       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:51:37.101950       1 handler.go:285] Adding GroupVersion  v1 to ResourceManager
I1126 03:51:37.102230       1 apis.go:112] API group "internal.apiserver.k8s.io" is not enabled, skipping.
W1126 03:51:37.103320       1 logging.go:55] [core] [Channel #95 SubChannel #96]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.110880       1 logging.go:55] [core] [Channel #99 SubChannel #100]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:37.117955       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:37.124877       1 logging.go:55] [core] [Channel #107 SubChannel #108]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.132521       1 logging.go:55] [core] [Channel #111 SubChannel #112]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.147877       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.155041       1 logging.go:55] [core] [Channel #119 SubChannel #120]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.165420       1 logging.go:55] [core] [Channel #123 SubChannel #124]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:37.175358       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:37.183101       1 logging.go:55] [core] [Channel #131 SubChannel #132]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:37.191612       1 logging.go:55] [core] [Channel #135 SubChannel #136]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:37.199534       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.210574       1 logging.go:55] [core] [Channel #143 SubChannel #144]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.221141       1 logging.go:55] [core] [Channel #147 SubChannel #148]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.239246       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.251256       1 logging.go:55] [core] [Channel #155 SubChannel #156]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.263153       1 logging.go:55] [core] [Channel #159 SubChannel #160]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.277581       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.289912       1 logging.go:55] [core] [Channel #167 SubChannel #168]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.301826       1 logging.go:55] [core] [Channel #171 SubChannel #172]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:37.316489       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.325567       1 logging.go:55] [core] [Channel #179 SubChannel #180]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.333076       1 logging.go:55] [core] [Channel #183 SubChannel #184]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.344875       1 logging.go:55] [core] [Channel #187 SubChannel #188]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 03:51:37.355230       1 apis.go:112] API group "storagemigration.k8s.io" is not enabled, skipping.
W1126 03:51:37.357993       1 logging.go:55] [core] [Channel #191 SubChannel #192]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.364964       1 logging.go:55] [core] [Channel #195 SubChannel #196]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.371671       1 logging.go:55] [core] [Channel #199 SubChannel #200]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:37.379366       1 logging.go:55] [core] [Channel #203 SubChannel #204]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:37.387579       1 logging.go:55] [core] [Channel #207 SubChannel #208]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:37.395949       1 logging.go:55] [core] [Channel #211 SubChannel #212]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.402654       1 logging.go:55] [core] [Channel #215 SubChannel #216]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.409493       1 logging.go:55] [core] [Channel #219 SubChannel #220]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.417846       1 logging.go:55] [core] [Channel #223 SubChannel #224]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.424294       1 logging.go:55] [core] [Channel #227 SubChannel #228]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.431227       1 logging.go:55] [core] [Channel #231 SubChannel #232]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1126 03:51:37.438900       1 logging.go:55] [core] [Channel #235 SubChannel #236]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.466510       1 logging.go:55] [core] [Channel #239 SubChannel #240]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.470924       1 logging.go:55] [core] [Channel #243 SubChannel #244]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.478967       1 logging.go:55] [core] [Channel #247 SubChannel #248]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1126 03:51:37.489991       1 logging.go:55] [core] [Channel #251 SubChannel #252]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1126 03:51:37.515417       1 handler.go:285] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W1126 03:51:37.515458       1 genericapiserver.go:784] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W1126 03:51:37.515466       1 genericapiserver.go:784] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I1126 03:51:37.515732       1 handler.go:285] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W1126 03:51:37.515761       1 genericapiserver.go:784] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I1126 03:51:37.516288       1 handler.go:285] Adding GroupVersion autoscaling v2 to ResourceManager
I1126 03:51:37.516797       1 handler.go:285] Adding GroupVersion autoscaling v1 to ResourceManager
W1126 03:51:37.516828       1 genericapiserver.go:784] Skipping API autoscaling/v2beta1 because it has no resources.
W1126 03:51:37.516834       1 genericapiserver.go:784] Skipping API autoscaling/v2beta2 because it has no resources.
I1126 03:51:37.517741       1 handler.go:285] Adding GroupVersion batch v1 to ResourceManager
W1126 03:51:37.517774       1 genericapiserver.go:784] Skipping API batch/v1beta1 because it has no resources.
I1126 03:51:37.518512       1 handler.go:285] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W1126 03:51:37.518543       1 genericapiserver.go:784] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W1126 03:51:37.518551       1 genericapiserver.go:784] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I1126 03:51:37.518963       1 handler.go:285] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W1126 03:51:37.518993       1 genericapiserver.go:784] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W1126 03:51:37.518999       1 genericapiserver.go:784] Skipping API coordination.k8s.io/v1alpha2 because it has no resources.
I1126 03:51:37.519380       1 handler.go:285] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
W1126 03:51:37.519417       1 genericapiserver.go:784] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I1126 03:51:37.521001       1 handler.go:285] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W1126 03:51:37.521040       1 genericapiserver.go:784] Skipping API networking.k8s.io/v1beta1 because it has no resources.
I1126 03:51:37.521461       1 handler.go:285] Adding GroupVersion node.k8s.io v1 to ResourceManager
W1126 03:51:37.521493       1 genericapiserver.go:784] Skipping API node.k8s.io/v1beta1 because it has no resources.
W1126 03:51:37.521499       1 genericapiserver.go:784] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I1126 03:51:37.522809       1 handler.go:285] Adding GroupVersion policy v1 to ResourceManager
W1126 03:51:37.522859       1 genericapiserver.go:784] Skipping API policy/v1beta1 because it has no resources.
I1126 03:51:37.524301       1 handler.go:285] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W1126 03:51:37.524342       1 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W1126 03:51:37.524353       1 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I1126 03:51:37.524656       1 handler.go:285] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W1126 03:51:37.524693       1 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W1126 03:51:37.524702       1 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I1126 03:51:37.526475       1 handler.go:285] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W1126 03:51:37.526512       1 genericapiserver.go:784] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W1126 03:51:37.526519       1 genericapiserver.go:784] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I1126 03:51:37.527398       1 handler.go:285] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
W1126 03:51:37.527451       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta3 because it has no resources.
W1126 03:51:37.527460       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
W1126 03:51:37.527463       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
I1126 03:51:37.530877       1 handler.go:285] Adding GroupVersion apps v1 to ResourceManager
W1126 03:51:37.530914       1 genericapiserver.go:784] Skipping API apps/v1beta2 because it has no resources.
W1126 03:51:37.530920       1 genericapiserver.go:784] Skipping API apps/v1beta1 because it has no resources.
I1126 03:51:37.532627       1 handler.go:285] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W1126 03:51:37.532660       1 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W1126 03:51:37.532666       1 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I1126 03:51:37.533127       1 handler.go:285] Adding GroupVersion events.k8s.io v1 to ResourceManager
W1126 03:51:37.533158       1 genericapiserver.go:784] Skipping API events.k8s.io/v1beta1 because it has no resources.
W1126 03:51:37.533217       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta2 because it has no resources.
I1126 03:51:37.534842       1 handler.go:285] Adding GroupVersion resource.k8s.io v1 to ResourceManager
W1126 03:51:37.534875       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta1 because it has no resources.
W1126 03:51:37.534881       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1alpha3 because it has no resources.
W1126 03:51:37.537825       1 logging.go:55] [core] [Channel #255 SubChannel #256]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1126 03:51:37.545216       1 handler.go:285] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1126 03:51:37.545259       1 genericapiserver.go:784] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1126 03:51:37.920707       1 secure_serving.go:211] Serving securely on [::]:6443
I1126 03:51:37.920754       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 03:51:37.920768       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1126 03:51:37.920780       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1126 03:51:37.920772       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/etc/kubernetes/pki/apiserver.crt::/etc/kubernetes/pki/apiserver.key"
I1126 03:51:37.921741       1 aggregator.go:169] waiting for initial CRD sync...
I1126 03:51:37.921806       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/etc/kubernetes/pki/front-proxy-client.crt::/etc/kubernetes/pki/front-proxy-client.key"
I1126 03:51:37.921961       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1126 03:51:37.922006       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1126 03:51:37.922041       1 controller.go:78] Starting OpenAPI AggregationController
I1126 03:51:37.925473       1 controller.go:119] Starting legacy_token_tracking_controller
I1126 03:51:37.925523       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I1126 03:51:37.925835       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1126 03:51:37.940444       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I1126 03:51:37.940633       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1126 03:51:37.940774       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1126 03:51:37.940951       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I1126 03:51:37.947818       1 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
I1126 03:51:37.965869       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I1126 03:51:37.969964       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I1126 03:51:37.950602       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1126 03:51:37.951107       1 controller.go:142] Starting OpenAPI controller
I1126 03:51:37.951139       1 controller.go:90] Starting OpenAPI V3 controller
I1126 03:51:37.951148       1 naming_controller.go:299] Starting NamingConditionController
I1126 03:51:37.951155       1 establishing_controller.go:81] Starting EstablishingController
I1126 03:51:37.951161       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1126 03:51:37.951169       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1126 03:51:37.951176       1 crd_finalizer.go:269] Starting CRDFinalizer
I1126 03:51:37.958289       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1126 03:51:37.921986       1 local_available_controller.go:156] Starting LocalAvailability controller
I1126 03:51:37.958624       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1126 03:51:37.969988       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1126 03:51:37.972965       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1126 03:51:37.969997       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1126 03:51:37.973160       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
I1126 03:51:37.973231       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1126 03:51:38.018447       1 repairip.go:210] Starting ipallocator-repair-controller
I1126 03:51:38.018651       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I1126 03:51:38.025550       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1126 03:51:38.032070       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1126 03:51:38.041804       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1126 03:51:38.069518       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1126 03:51:38.072845       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1126 03:51:38.072933       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1126 03:51:38.074099       1 cache.go:39] Caches are synced for LocalAvailability controller
I1126 03:51:38.074138       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1126 03:51:38.074164       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1126 03:51:38.074170       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1126 03:51:38.074270       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1126 03:51:38.074321       1 aggregator.go:171] initial CRD sync complete...
I1126 03:51:38.074330       1 autoregister_controller.go:144] Starting autoregister controller
I1126 03:51:38.074334       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1126 03:51:38.074339       1 cache.go:39] Caches are synced for autoregister controller
I1126 03:51:38.083156       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1126 03:51:38.083207       1 policy_source.go:240] refreshing policies
I1126 03:51:38.119413       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1126 03:51:38.123009       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1126 03:51:38.128449       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1126 03:51:38.139196       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1126 03:51:38.926232       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W1126 03:51:39.242631       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.150.131]
I1126 03:51:39.243891       1 controller.go:667] quota admission added evaluator for: endpoints
I1126 03:51:39.249272       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1126 03:52:04.146479       1 controller.go:667] quota admission added evaluator for: serviceaccounts
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
NAME          STATUS   ROLES           AGE   VERSION
terramaster   Ready    control-plane   21h   v1.34.2
terranode1    Ready    <none>          21h   v1.34.2
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get csr
NAME        AGE     SIGNERNAME                                    REQUESTOR                 REQUESTEDDURATION   CONDITION
csr-42vq2   21h     kubernetes.io/kube-apiserver-client-kubelet   system:node:terramaster   <none>              Approved,Issued
csr-7zgv8   21h     kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:m9j5zh   <none>              Approved,Issued
csr-xbq96   8m13s   kubernetes.io/kubelet-serving                 system:node:terranode1    <none>              Pending
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl certificate approve csr-xbq96
certificatesigningrequest.certificates.k8s.io/csr-xbq96 approved
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get csr
NAME        AGE    SIGNERNAME                                    REQUESTOR                 REQUESTEDDURATION   CONDITION
csr-42vq2   21h    kubernetes.io/kube-apiserver-client-kubelet   system:node:terramaster   <none>              Approved,Issued
csr-7zgv8   21h    kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:m9j5zh   <none>              Approved,Issued
csr-xbq96   9m4s   kubernetes.io/kubelet-serving                 system:node:terranode1    <none>              Approved,Issued
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl logs kube-bench-4lzdp
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
d6e72761fad49       a5f569d49a979       About a minute ago   Running             kube-apiserver            4                   4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
9a12c9526f7fa       a5f569d49a979       5 minutes ago        Exited              kube-apiserver            3                   4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ # 1. หา Path ของ Log file
sudo ls -d /var/log/pods/kube-system_kube-apiserver*

# 2. เข้าไปในโฟลเดอร์นั้น (เปลี่ยน path ตามที่เจอ)
cd /var/log/pods/kube-system_kube-apiserver-terramaster_xxxx.../kube-apiserver/

# 3. ดูไฟล์ Log ล่าสุด (เช่น 0.log หรือ 1.log)
ls -l
sudo tail -n 100 0.log
/var/log/pods/kube-system_kube-apiserver-terramaster_37e33c21297559364500fcaf7e1ce84f
-bash: cd: /var/log/pods/kube-system_kube-apiserver-terramaster_xxxx.../kube-apiserver/: No such file or directory
total 192
drwxrwxr-x 43 master master  4096 Nov 25 03:22 cfg
drwxrwxr-x  2 master master  4096 Nov 25 03:22 check
drwxrwxr-x  3 master master  4096 Nov 25 03:22 cmd
-rw-rw-r--  1 master master   149 Nov 25 03:22 codecov.yml
-rw-rw-r--  1 master master  5215 Nov 25 03:22 CONTRIBUTING.md
-rw-rw-r--  1 master master  2863 Nov 25 03:22 Dockerfile
-rw-rw-r--  1 master master  2415 Nov 25 03:22 Dockerfile.fips.ubi
-rw-rw-r--  1 master master  2356 Nov 25 03:22 Dockerfile.ubi
drwxrwxr-x  3 master master  4096 Nov 25 03:22 docs
-rwxrwxr-x  1 master master   642 Nov 26 02:48 entrypoint.sh
-rw-rw-r--  1 master master    71 Nov 25 03:22 fipsonly.go
-rw-rw-r--  1 master master  4386 Nov 25 03:22 go.mod
-rw-rw-r--  1 master master 27582 Nov 25 03:22 go.sum
drwxrwxr-x  2 master master  4096 Nov 25 03:22 hack
drwxrwxr-x  2 master master  4096 Nov 26 02:48 helper_scripts
drwxrwxr-x  2 master master  4096 Nov 25 03:22 hooks
drwxrwxr-x  3 master master  4096 Nov 25 03:22 integration
drwxrwxr-x  3 master master  4096 Nov 25 03:22 internal
-rw-rw-r--  1 master master  1068 Nov 25 03:22 job-ack.yaml
-rw-rw-r--  1 master master  1124 Nov 25 03:22 job-aks.yaml
-rw-rw-r--  1 master master  2201 Nov 25 03:22 job-eks-asff.yaml
-rw-rw-r--  1 master master  1239 Nov 25 03:22 job-eks-stig.yaml
-rw-rw-r--  1 master master  1311 Nov 25 03:22 job-eks.yaml
-rw-rw-r--  1 master master  1266 Nov 25 03:22 job-gke.yaml
-rw-rw-r--  1 master master  1012 Nov 25 03:22 job-iks.yaml
-rw-rw-r--  1 master master  3878 Nov 25 03:22 job-master.yaml
-rw-rw-r--  1 master master  2908 Nov 25 03:22 job-node.yaml
-rw-rw-r--  1 master master  1880 Nov 25 03:22 job-tkgi.yaml
-rw-rw-r--  1 master master  2685 Nov 25 03:22 job.yaml
-rw-rw-r--  1 master master 11358 Nov 25 03:22 LICENSE
-rw-rw-r--  1 master master   726 Nov 25 03:22 main.go
-rw-rw-r--  1 master master  4142 Nov 25 03:22 makefile
-rw-rw-r--  1 master master   939 Nov 25 03:22 mkdocs.yml
-rw-rw-r--  1 master master   143 Nov 25 03:22 NOTICE
-rw-rw-r--  1 master master    32 Nov 25 03:22 OWNERS
-rw-rw-r--  1 master master  4412 Nov 25 03:22 README.md
tail: cannot open '0.log' for reading: No such file or directory
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ cd /var/log/pods/kube-system_kube-apiserver-terramaster_37e33c21297559364500fcaf7e1ce84f/kube-apiserver/
master@terramaster:/var/log/pods/kube-system_kube-apiserver-terramaster_37e33c21297559364500fcaf7e1ce84f/kube-apiserver$ ls -l
total 108
-rw-r----- 1 root root 56987 Nov 26 03:55 3.log
-rw-r----- 1 root root 51087 Nov 26 03:56 4.log
master@terramaster:/var/log/pods/kube-system_kube-apiserver-terramaster_37e33c21297559364500fcaf7e1ce84f/kube-apiserver$ sudo tail -n 100 4.log               2025-11-26T03:56:08.268405618Z stderr F W1126 03:56:08.268266       1 genericapiserver.go:784] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
2025-11-26T03:56:08.272103475Z stderr F I1126 03:56:08.271832       1 handler.go:285] Adding GroupVersion networking.k8s.io v1 to ResourceManager
2025-11-26T03:56:08.272152492Z stderr F W1126 03:56:08.271897       1 genericapiserver.go:784] Skipping API networking.k8s.io/v1beta1 because it has no resources.
2025-11-26T03:56:08.272897853Z stderr F I1126 03:56:08.272596       1 handler.go:285] Adding GroupVersion node.k8s.io v1 to ResourceManager
2025-11-26T03:56:08.272915486Z stderr F W1126 03:56:08.272643       1 genericapiserver.go:784] Skipping API node.k8s.io/v1beta1 because it has no resources.
2025-11-26T03:56:08.272922177Z stderr F W1126 03:56:08.272652       1 genericapiserver.go:784] Skipping API node.k8s.io/v1alpha1 because it has no resources.
2025-11-26T03:56:08.273954765Z stderr F I1126 03:56:08.273678       1 handler.go:285] Adding GroupVersion policy v1 to ResourceManager
2025-11-26T03:56:08.273968887Z stderr F W1126 03:56:08.273724       1 genericapiserver.go:784] Skipping API policy/v1beta1 because it has no resources.
2025-11-26T03:56:08.275958355Z stderr F I1126 03:56:08.275677       1 handler.go:285] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
2025-11-26T03:56:08.275971644Z stderr F W1126 03:56:08.275725       1 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
2025-11-26T03:56:08.27599222Z stderr F W1126 03:56:08.275735       1 genericapiserver.go:784] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
2025-11-26T03:56:08.27658551Z stderr F I1126 03:56:08.276364       1 handler.go:285] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
2025-11-26T03:56:08.276601376Z stderr F W1126 03:56:08.276410       1 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
2025-11-26T03:56:08.276607551Z stderr F W1126 03:56:08.276420       1 genericapiserver.go:784] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
2025-11-26T03:56:08.280145955Z stderr F I1126 03:56:08.279821       1 handler.go:285] Adding GroupVersion storage.k8s.io v1 to ResourceManager
2025-11-26T03:56:08.280171025Z stderr F W1126 03:56:08.279890       1 genericapiserver.go:784] Skipping API storage.k8s.io/v1beta1 because it has no resources.
2025-11-26T03:56:08.280175017Z stderr F W1126 03:56:08.279903       1 genericapiserver.go:784] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
2025-11-26T03:56:08.281802639Z stderr F I1126 03:56:08.281547       1 handler.go:285] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
2025-11-26T03:56:08.281820971Z stderr F W1126 03:56:08.281598       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta3 because it has no resources.
2025-11-26T03:56:08.281826095Z stderr F W1126 03:56:08.281608       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
2025-11-26T03:56:08.281830876Z stderr F W1126 03:56:08.281612       1 genericapiserver.go:784] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
2025-11-26T03:56:08.287327423Z stderr F I1126 03:56:08.286962       1 handler.go:285] Adding GroupVersion apps v1 to ResourceManager
2025-11-26T03:56:08.287354764Z stderr F W1126 03:56:08.287032       1 genericapiserver.go:784] Skipping API apps/v1beta2 because it has no resources.
2025-11-26T03:56:08.287358628Z stderr F W1126 03:56:08.287044       1 genericapiserver.go:784] Skipping API apps/v1beta1 because it has no resources.
2025-11-26T03:56:08.290384421Z stderr F I1126 03:56:08.290044       1 handler.go:285] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
2025-11-26T03:56:08.290418629Z stderr F W1126 03:56:08.290103       1 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
2025-11-26T03:56:08.290427831Z stderr F W1126 03:56:08.290114       1 genericapiserver.go:784] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
2025-11-26T03:56:08.291222685Z stderr F I1126 03:56:08.290908       1 handler.go:285] Adding GroupVersion events.k8s.io v1 to ResourceManager
2025-11-26T03:56:08.291244909Z stderr F W1126 03:56:08.290961       1 genericapiserver.go:784] Skipping API events.k8s.io/v1beta1 because it has no resources.
2025-11-26T03:56:08.291251588Z stderr F W1126 03:56:08.291059       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta2 because it has no resources.
2025-11-26T03:56:08.293824468Z stderr F I1126 03:56:08.293545       1 handler.go:285] Adding GroupVersion resource.k8s.io v1 to ResourceManager
2025-11-26T03:56:08.29384218Z stderr F W1126 03:56:08.293597       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1beta1 because it has no resources.
2025-11-26T03:56:08.293846512Z stderr F W1126 03:56:08.293607       1 genericapiserver.go:784] Skipping API resource.k8s.io/v1alpha3 because it has no resources.
2025-11-26T03:56:08.2973441Z stderr F W1126 03:56:08.297198       1 logging.go:55] [core] [Channel #255 SubChannel #256]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
2025-11-26T03:56:08.306115126Z stderr F I1126 03:56:08.305906       1 handler.go:285] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
2025-11-26T03:56:08.306138463Z stderr F W1126 03:56:08.306054       1 genericapiserver.go:784] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
2025-11-26T03:56:08.73473761Z stderr F I1126 03:56:08.734508       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
2025-11-26T03:56:08.734936529Z stderr F I1126 03:56:08.734524       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
2025-11-26T03:56:08.734962672Z stderr F I1126 03:56:08.734729       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/etc/kubernetes/pki/apiserver.crt::/etc/kubernetes/pki/apiserver.key"
2025-11-26T03:56:08.737624113Z stderr F I1126 03:56:08.734997       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
2025-11-26T03:56:08.737641502Z stderr F I1126 03:56:08.734938       1 secure_serving.go:211] Serving securely on [::]:6443
2025-11-26T03:56:08.7376476Z stderr F I1126 03:56:08.735376       1 apf_controller.go:377] Starting API Priority and Fairness config controller
2025-11-26T03:56:08.737653314Z stderr F I1126 03:56:08.735706       1 aggregator.go:169] waiting for initial CRD sync...
2025-11-26T03:56:08.737658051Z stderr F I1126 03:56:08.735769       1 controller.go:78] Starting OpenAPI AggregationController
2025-11-26T03:56:08.73766245Z stderr F I1126 03:56:08.735895       1 controller.go:80] Starting OpenAPI V3 AggregationController
2025-11-26T03:56:08.737666886Z stderr F I1126 03:56:08.736005       1 customresource_discovery_controller.go:294] Starting DiscoveryController
2025-11-26T03:56:08.737672079Z stderr F I1126 03:56:08.736291       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
2025-11-26T03:56:08.737676504Z stderr F I1126 03:56:08.736342       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
2025-11-26T03:56:08.73768131Z stderr F I1126 03:56:08.736389       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/etc/kubernetes/pki/front-proxy-client.crt::/etc/kubernetes/pki/front-proxy-client.key"
2025-11-26T03:56:08.737685638Z stderr F I1126 03:56:08.736509       1 remote_available_controller.go:425] Starting RemoteAvailability controller
2025-11-26T03:56:08.737690013Z stderr F I1126 03:56:08.736575       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
2025-11-26T03:56:08.737694243Z stderr F I1126 03:56:08.736691       1 controller.go:119] Starting legacy_token_tracking_controller
2025-11-26T03:56:08.737699123Z stderr F I1126 03:56:08.736733       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
2025-11-26T03:56:08.737706325Z stderr F I1126 03:56:08.736754       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
2025-11-26T03:56:08.737711834Z stderr F I1126 03:56:08.736791       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
2025-11-26T03:56:08.737716355Z stderr F I1126 03:56:08.736808       1 controller.go:142] Starting OpenAPI controller
2025-11-26T03:56:08.737720907Z stderr F I1126 03:56:08.736851       1 controller.go:90] Starting OpenAPI V3 controller
2025-11-26T03:56:08.737725495Z stderr F I1126 03:56:08.736893       1 naming_controller.go:299] Starting NamingConditionController
2025-11-26T03:56:08.737730149Z stderr F I1126 03:56:08.736924       1 establishing_controller.go:81] Starting EstablishingController
2025-11-26T03:56:08.737734493Z stderr F I1126 03:56:08.736953       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
2025-11-26T03:56:08.737738864Z stderr F I1126 03:56:08.736979       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
2025-11-26T03:56:08.737756856Z stderr F I1126 03:56:08.737005       1 crd_finalizer.go:269] Starting CRDFinalizer
2025-11-26T03:56:08.737763109Z stderr F I1126 03:56:08.737043       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
2025-11-26T03:56:08.737767786Z stderr F I1126 03:56:08.737134       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
2025-11-26T03:56:08.737772356Z stderr F I1126 03:56:08.737375       1 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
2025-11-26T03:56:08.737777294Z stderr F I1126 03:56:08.737441       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
2025-11-26T03:56:08.78737693Z stderr F I1126 03:56:08.737490       1 local_available_controller.go:156] Starting LocalAvailability controller
2025-11-26T03:56:08.787629573Z stderr F I1126 03:56:08.787459       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
2025-11-26T03:56:08.789319899Z stderr F I1126 03:56:08.736939       1 repairip.go:210] Starting ipallocator-repair-controller
2025-11-26T03:56:08.789561158Z stderr F I1126 03:56:08.789418       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
2025-11-26T03:56:08.789806823Z stderr F I1126 03:56:08.736704       1 system_namespaces_controller.go:66] Starting system namespaces controller
2025-11-26T03:56:08.789979878Z stderr F I1126 03:56:08.737676       1 crdregistration_controller.go:114] Starting crd-autoregister controller
2025-11-26T03:56:08.790214155Z stderr F I1126 03:56:08.790051       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
2025-11-26T03:56:08.790373324Z stderr F I1126 03:56:08.736580       1 gc_controller.go:78] Starting apiserver lease garbage collector
2025-11-26T03:56:08.863352455Z stderr F I1126 03:56:08.863141       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
2025-11-26T03:56:08.868430435Z stderr F I1126 03:56:08.868194       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
2025-11-26T03:56:08.868721048Z stderr F I1126 03:56:08.868599       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
2025-11-26T03:56:08.888443477Z stderr F I1126 03:56:08.888199       1 cache.go:39] Caches are synced for LocalAvailability controller
2025-11-26T03:56:08.889951324Z stderr F I1126 03:56:08.889781       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
2025-11-26T03:56:08.891150143Z stderr F I1126 03:56:08.891006       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
2025-11-26T03:56:08.8911751Z stderr F I1126 03:56:08.891109       1 aggregator.go:171] initial CRD sync complete...
2025-11-26T03:56:08.891397031Z stderr F I1126 03:56:08.891249       1 autoregister_controller.go:144] Starting autoregister controller
2025-11-26T03:56:08.891408523Z stderr F I1126 03:56:08.891297       1 cache.go:32] Waiting for caches to sync for autoregister controller
2025-11-26T03:56:08.891412427Z stderr F I1126 03:56:08.891309       1 cache.go:39] Caches are synced for autoregister controller
2025-11-26T03:56:08.920953327Z stderr F I1126 03:56:08.920654       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
2025-11-26T03:56:08.928400932Z stderr F I1126 03:56:08.927756       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
2025-11-26T03:56:08.928433077Z stderr F I1126 03:56:08.927840       1 policy_source.go:240] refreshing policies
2025-11-26T03:56:08.936189403Z stderr F I1126 03:56:08.935921       1 apf_controller.go:382] Running API Priority and Fairness config worker
2025-11-26T03:56:08.936231618Z stderr F I1126 03:56:08.935983       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
2025-11-26T03:56:08.936917243Z stderr F I1126 03:56:08.936805       1 cache.go:39] Caches are synced for RemoteAvailability controller
2025-11-26T03:56:08.936984524Z stderr F I1126 03:56:08.936841       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
2025-11-26T03:56:08.938092468Z stderr F I1126 03:56:08.937951       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
2025-11-26T03:56:08.938187096Z stderr F I1126 03:56:08.938019       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
2025-11-26T03:56:08.944421081Z stderr F I1126 03:56:08.944098       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
2025-11-26T03:56:08.9818353Z stderr F I1126 03:56:08.981643       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
2025-11-26T03:56:09.739888668Z stderr F I1126 03:56:09.739667       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
2025-11-26T03:56:10.050570269Z stderr F W1126 03:56:10.050179       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.150.131]
2025-11-26T03:56:10.053273437Z stderr F I1126 03:56:10.052449       1 controller.go:667] quota admission added evaluator for: endpoints
2025-11-26T03:56:10.0651524Z stderr F I1126 03:56:10.064928       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
2025-11-26T03:56:46.812184629Z stderr F I1126 03:56:46.811907       1 controller.go:667] quota admission added evaluator for: serviceaccounts
master@terramaster:/var/log/pods/kube-system_kube-apiserver-terramaster_37e33c21297559364500fcaf7e1ce84f/kube-apiserver$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
d6e72761fad49       a5f569d49a979       4 minutes ago       Running             kube-apiserver            4                   4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
9a12c9526f7fa       a5f569d49a979       8 minutes ago       Exited              kube-apiserver            3                   4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
master@terramaster:/var/log/pods/kube-system_kube-apiserver-terramaster_37e33c21297559364500fcaf7e1ce84f/kube-apiserver$ sudo ss -tulpn | grep 6443
master@terramaster:/var/log/pods/kube-system_kube-apiserver-terramaster_37e33c21297559364500fcaf7e1ce84f/kube-apiserver$ sudo ss -tulpn | grep 6443
master@terramaster:/var/log/pods/kube-system_kube-apiserver-terramaster_37e33c21297559364500fcaf7e1ce84f/kube-apiserver$ # 1. ลบ config เก่าทิ้ง
rm -rf $HOME/.kube/config

# 2. ก๊อปปี้อันใหม่จาก admin.conf (ที่ Kubeadm สร้างให้ล่าสุด)
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 3. ลองยิงใหม่
kubectl get nodes
NAME          STATUS   ROLES           AGE   VERSION
terramaster   Ready    control-plane   21h   v1.34.2
terranode1    Ready    <none>          21h   v1.34.2
master@terramaster:/var/log/pods/kube-system_kube-apiserver-terramaster_37e33c21297559364500fcaf7e1ce84f/kube-apiserver$ cd /
bin/        cdrom/      etc/        lib/        lib64/      lost+found/ mnt/        proc/       run/        snap/       sys/        usr/
boot/       dev/        home/       lib32/      libx32/     media/      opt/        root/       sbin/       srv/        tmp/        var/
master@terramaster:/var/log/pods/kube-system_kube-apiserver-terramaster_37e33c21297559364500fcaf7e1ce84f/kube-apiserver$ cd /etc/m
mdadm/          menu/           menu-methods/   modprobe.d/     modules-load.d/ monit/          multipath/
master@terramaster:/var/log/pods/kube-system_kube-apiserver-terramaster_37e33c21297559364500fcaf7e1ce84f/kube-apiserver$ cd /home/master/
master@terramaster:~$ cd /home/master/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench/
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get pods
NAME               READY   STATUS      RESTARTS   AGE
kube-bench-4lzdp   0/1     Completed   0          19m
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl logs kube-bench-4lzdp
[INFO] 4 Worker Node Security Configuration
[INFO] 4.1 Worker Node Configuration Files
[FAIL] 4.1.1 Ensure that the kubelet service file permissions are set to 600 or more restrictive (Automated)
[PASS] 4.1.2 Ensure that the kubelet service file ownership is set to root:root (Automated)
[WARN] 4.1.3 If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive (Manual)
[WARN] 4.1.4 If proxy kubeconfig file exists ensure ownership is set to root:root (Manual)
[PASS] 4.1.5 Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive (Automated)
[PASS] 4.1.6 Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root (Automated)
[PASS] 4.1.7 Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Manual)
[PASS] 4.1.8 Ensure that the client certificate authorities file ownership is set to root:root (Manual)
[FAIL] 4.1.9 If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive (Automated)
[PASS] 4.1.10 If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root (Automated)
[INFO] 4.2 Kubelet
[PASS] 4.2.1 Ensure that the --anonymous-auth argument is set to false (Automated)
[PASS] 4.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated)
[PASS] 4.2.3 Ensure that the --client-ca-file argument is set as appropriate (Automated)
[PASS] 4.2.4 Verify that if defined, the --read-only-port argument is set to 0 (Manual)
[PASS] 4.2.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Manual)
[PASS] 4.2.6 Ensure that the --make-iptables-util-chains argument is set to true (Automated)
[PASS] 4.2.7 Ensure that the --hostname-override argument is not set (Manual)
[PASS] 4.2.8 Ensure that the eventRecordQPS argument is set to a level which ensures appropriate event capture (Manual)
[WARN] 4.2.9 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Manual)
[PASS] 4.2.10 Ensure that the --rotate-certificates argument is not set to false (Automated)
[PASS] 4.2.11 Verify that the RotateKubeletServerCertificate argument is set to true (Manual)
[WARN] 4.2.12 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Manual)
[WARN] 4.2.13 Ensure that a limit is set on pod PIDs (Manual)
[WARN] 4.2.14 Ensure that the --seccomp-default parameter is set to true (Manual)
[WARN] 4.2.15 Ensure that the --IPAddressDeny is set to any (Manual)
[INFO] 4.3 kube-proxy
[PASS] 4.3.1 Ensure that the kube-proxy metrics service is bound to localhost (Automated)

== Remediations node ==
4.1.1 Run the below command (based on the file location on your system) on the each worker node.
For example, chmod 600 /lib/systemd/system/kubelet.service

4.1.3 Run the below command (based on the file location on your system) on the each worker node.
For example,
chmod 600 /etc/kubernetes/proxy.conf

4.1.4 Run the below command (based on the file location on your system) on the each worker node.
For example, chown root:root /etc/kubernetes/proxy.conf

4.1.9 Run the following command (using the config file location identified in the Audit step)
chmod 600 /var/lib/kubelet/config.yaml

4.2.9 If using a Kubelet config file, edit the file to set `tlsCertFile` to the location
of the certificate file to use to identify this Kubelet, and `tlsPrivateKeyFile`
to the location of the corresponding private key file.
If using command line arguments, edit the kubelet service file
/lib/systemd/system/kubelet.service on each worker node and
set the below parameters in KUBELET_CERTIFICATE_ARGS variable.
--tls-cert-file=<path/to/tls-certificate-file>
--tls-private-key-file=<path/to/tls-key-file>
Based on your system, restart the kubelet service. For example,
systemctl daemon-reload
systemctl restart kubelet.service

4.2.12 If using a Kubelet config file, edit the file to set `tlsCipherSuites` to
TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
or to a subset of these values.
If using executable arguments, edit the kubelet service file
/lib/systemd/system/kubelet.service on each worker node and
set the --tls-cipher-suites parameter as follows, or to a subset of these values.
--tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
Based on your system, restart the kubelet service. For example:
systemctl daemon-reload
systemctl restart kubelet.service

4.2.13 Decide on an appropriate level for this parameter and set it,
either via the --pod-max-pids command line parameter or the PodPidsLimit configuration file setting.

4.2.14 Set the parameter, either via the --seccomp-default command line parameter or the
seccompDefault configuration file setting.
By default the seccomp profile is not enabled.

4.2.15 Configuring the setting IPAddressDeny=any will deny service to any IP address not specified in the complimentary setting IPAddressAllow configuration parameter (
IPAddressDeny=any
IPAddressAllow={{ kubelet_secure_addresses }}
*Note
  kubelet_secure_addresses: "localhost link-local {{ kube_pods_subnets |regex_replace(',', ' ') }} {{ kube_node_addresses }} {{ loadbalancer_apiserver.address | default('')"
By default IPAddressDeny is not enabled.


== Summary node ==
17 checks PASS
2 checks FAIL
7 checks WARN
0 checks INFO

[INFO] 5 Kubernetes Policies
[INFO] 5.1 RBAC and Service Accounts
[WARN] 5.1.1 Ensure that the cluster-admin role is only used where required (Manual)
[WARN] 5.1.2 Minimize access to secrets (Manual)
[WARN] 5.1.3 Minimize wildcard use in Roles and ClusterRoles (Manual)
[WARN] 5.1.4 Minimize access to create pods (Manual)
[WARN] 5.1.5 Ensure that default service accounts are not actively used (Manual)
[WARN] 5.1.6 Ensure that Service Account Tokens are only mounted where necessary (Manual)
[WARN] 5.1.7 Avoid use of system:masters group (Manual)
[WARN] 5.1.8 Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster (Manual)
[WARN] 5.1.9 Minimize access to create persistent volumes (Manual)
[WARN] 5.1.10 Minimize access to the proxy sub-resource of nodes (Manual)
[WARN] 5.1.11 Minimize access to the approval sub-resource of certificatesigningrequests objects (Manual)
[WARN] 5.1.12 Minimize access to webhook configuration objects (Manual)
[WARN] 5.1.13 Minimize access to the service account token creation (Manual)
[INFO] 5.2 Pod Security Standards
[WARN] 5.2.1 Ensure that the cluster has at least one active policy control mechanism in place (Manual)
[WARN] 5.2.2 Minimize the admission of privileged containers (Manual)
[WARN] 5.2.3 Minimize the admission of containers wishing to share the host process ID namespace (Manual)
[WARN] 5.2.4 Minimize the admission of containers wishing to share the host IPC namespace (Manual)
[WARN] 5.2.5 Minimize the admission of containers wishing to share the host network namespace (Manual)
[WARN] 5.2.6 Minimize the admission of containers with allowPrivilegeEscalation (Manual)
[WARN] 5.2.7 Minimize the admission of root containers (Manual)
[WARN] 5.2.8 Minimize the admission of containers with the NET_RAW capability (Manual)
[WARN] 5.2.9 Minimize the admission of containers with added capabilities (Manual)
[WARN] 5.2.10 Minimize the admission of containers with capabilities assigned (Manual)
[WARN] 5.2.11 Minimize the admission of Windows HostProcess containers (Manual)
[WARN] 5.2.12 Minimize the admission of HostPath volumes (Manual)
[WARN] 5.2.13 Minimize the admission of containers which use HostPorts (Manual)
[INFO] 5.3 Network Policies and CNI
[WARN] 5.3.1 Ensure that the CNI in use supports NetworkPolicies (Manual)
[WARN] 5.3.2 Ensure that all Namespaces have NetworkPolicies defined (Manual)
[INFO] 5.4 Secrets Management
[WARN] 5.4.1 Prefer using Secrets as files over Secrets as environment variables (Manual)
[WARN] 5.4.2 Consider external secret storage (Manual)
[INFO] 5.5 Extensible Admission Control
[WARN] 5.5.1 Configure Image Provenance using ImagePolicyWebhook admission controller (Manual)
[INFO] 5.6 General Policies
[WARN] 5.6.1 Create administrative boundaries between resources using namespaces (Manual)
[WARN] 5.6.2 Ensure that the seccomp profile is set to docker/default in your Pod definitions (Manual)
[WARN] 5.6.3 Apply SecurityContext to your Pods and Containers (Manual)
[WARN] 5.6.4 The default namespace should not be used (Manual)

== Remediations policies ==
5.1.1 Identify all clusterrolebindings to the cluster-admin role. Check if they are used and
if they need this role or if they could use a role with fewer privileges.
Where possible, first bind users to a lower privileged role and then remove the
clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name]
Condition: is_compliant is false if rolename is not cluster-admin and rolebinding is cluster-admin.

5.1.2 Where possible, remove get, list and watch access to Secret objects in the cluster.

5.1.3 Where possible replace any use of wildcards ["*"] in roles and clusterroles with specific
objects or actions.
Condition: role_is_compliant is false if ["*"] is found in rules.
Condition: clusterrole_is_compliant is false if ["*"] is found in rules.

5.1.4 Where possible, remove create access to pod objects in the cluster.

5.1.5 Create explicit service accounts wherever a Kubernetes workload requires specific access
to the Kubernetes API server.
Modify the configuration of each default service account to include this value
`automountServiceAccountToken: false`.

5.1.6 Modify the definition of ServiceAccounts and Pods which do not need to mount service
account tokens to disable it, with `automountServiceAccountToken: false`.
If both the ServiceAccount and the Pod's .spec specify a value for automountServiceAccountToken, the Pod spec takes precedence.
Condition: Pod is_compliant to true when
  - ServiceAccount is automountServiceAccountToken: false and Pod is automountServiceAccountToken: false or notset
  - ServiceAccount is automountServiceAccountToken: true notset and Pod is automountServiceAccountToken: false

5.1.7 Remove the system:masters group from all users in the cluster.

5.1.8 Where possible, remove the impersonate, bind and escalate rights from subjects.

5.1.9 Where possible, remove create access to PersistentVolume objects in the cluster.

5.1.10 Where possible, remove access to the proxy sub-resource of node objects.

5.1.11 Where possible, remove access to the approval sub-resource of certificatesigningrequests objects.

5.1.12 Where possible, remove access to the validatingwebhookconfigurations or mutatingwebhookconfigurations objects

5.1.13 Where possible, remove access to the token sub-resource of serviceaccount objects.

5.2.1 Ensure that either Pod Security Admission or an external policy control system is in place
for every namespace which contains user workloads.

5.2.2 Add policies to each namespace in the cluster which has user workloads to restrict the
admission of privileged containers.
Audit: the audit list all pods' containers to retrieve their .securityContext.privileged value.
Condition: is_compliant is false if container's `.securityContext.privileged` is set to `true`.
Default: by default, there are no restrictions on the creation of privileged containers.

5.2.3 Add policies to each namespace in the cluster which has user workloads to restrict the
admission of `hostPID` containers.
Audit: the audit retrieves each Pod' spec.hostPID.
Condition: is_compliant is false if Pod's spec.hostPID is set to `true`.
Default: by default, there are no restrictions on the creation of hostPID containers.

5.2.4 Add policies to each namespace in the cluster which has user workloads to restrict the
admission of `hostIPC` containers.
Audit: the audit retrieves each Pod' spec.IPC.
Condition: is_compliant is false if Pod's spec.hostIPC is set to `true`.
Default: by default, there are no restrictions on the creation of hostIPC containers.

5.2.5 Add policies to each namespace in the cluster which has user workloads to restrict the
admission of `hostNetwork` containers.
Audit: the audit retrieves each Pod' spec.hostNetwork.
Condition: is_compliant is false if Pod's spec.hostNetwork is set to `true`.
Default: by default, there are no restrictions on the creation of hostNetwork containers.

5.2.6 Add policies to each namespace in the cluster which has user workloads to restrict the
admission of containers with `.securityContext.allowPrivilegeEscalation` set to `true`.
Audit: the audit retrieves each Pod's container(s) `.securityContext.allowPrivilegeEscalation`.
Condition: is_compliant is false if container's `.securityContext.allowPrivilegeEscalation` is set to `true`.
Default: If notset, privilege escalation is allowed (default to true). However if PSP/PSA is used with a `restricted` profile,
privilege escalation is explicitly disallowed unless configured otherwise.

5.2.7 Create a policy for each namespace in the cluster, ensuring that either `MustRunAsNonRoot`
or `MustRunAs` with the range of UIDs not including 0, is set.

5.2.8 Add policies to each namespace in the cluster which has user workloads to restrict the
admission of containers with the `NET_RAW` capability.

5.2.9 Ensure that `allowedCapabilities` is not present in policies for the cluster unless
it is set to an empty array.
Audit: the audit retrieves each Pod's container(s) added capabilities.
Condition: is_compliant is false if added capabilities are added for a given container.
Default: Containers run with a default set of capabilities as assigned by the Container Runtime.

5.2.10 Review the use of capabilites in applications running on your cluster. Where a namespace
contains applications which do not require any Linux capabities to operate consider adding
a PSP which forbids the admission of containers which do not drop all capabilities.

5.2.11 Add policies to each namespace in the cluster which has user workloads to restrict the
admission of containers that have `.securityContext.windowsOptions.hostProcess` set to `true`.

5.2.12 Add policies to each namespace in the cluster which has user workloads to restrict the
admission of containers with `hostPath` volumes.

5.2.13 Add policies to each namespace in the cluster which has user workloads to restrict the
admission of containers which use `hostPort` sections.

5.3.1 If the CNI plugin in use does not support network policies, consideration should be given to
making use of a different plugin, or finding an alternate mechanism for restricting traffic
in the Kubernetes cluster.

5.3.2 Follow the documentation and create NetworkPolicy objects as you need them.

5.4.1 If possible, rewrite application code to read Secrets from mounted secret files, rather than
from environment variables.

5.4.2 Refer to the Secrets management options offered by your cloud provider or a third-party
secrets management solution.

5.5.1 Follow the Kubernetes documentation and setup image provenance.

5.6.1 Follow the documentation and create namespaces for objects in your deployment as you need
them.

5.6.2 Use `securityContext` to enable the docker/default seccomp profile in your pod definitions.
An example is as below:
  securityContext:
    seccompProfile:
      type: RuntimeDefault

5.6.3 Follow the Kubernetes documentation and apply SecurityContexts to your Pods. For a
suggested list of SecurityContexts, you may refer to the CIS Security Benchmark for Docker
Containers.

5.6.4 Ensure that namespaces are created to allow for appropriate segregation of Kubernetes
resources and that all new resources are created in a specific namespace.


== Summary policies ==
0 checks PASS
0 checks FAIL
35 checks WARN
0 checks INFO

== Summary total ==
17 checks PASS
2 checks FAIL
42 checks WARN
0 checks INFO

master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get pods
NAME               READY   STATUS      RESTARTS   AGE
kube-bench-4lzdp   0/1     Completed   0          51m
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
NAME          STATUS   ROLES           AGE   VERSION
terramaster   Ready    control-plane   22h   v1.34.2
terranode1    Ready    <none>          22h   v1.34.2
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD                                   NAMESPACE
3b65f51adce11       a5f569d49a979       About a minute ago   Running             kube-apiserver            10                  4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
2ead122b68e79       01e8bacf0f500       6 minutes ago        Running             kube-controller-manager   9                   eed392ff39793       kube-controller-manager-terramaster   kube-system
96b2ba633b2e0       88320b5498ff2       6 minutes ago        Running             kube-scheduler            9                   352f9ed56f913       kube-scheduler-terramaster            kube-system
c6778d82d4944       88320b5498ff2       10 minutes ago       Exited              kube-scheduler            8                   352f9ed56f913       kube-scheduler-terramaster            kube-system
5e356ea1de38d       01e8bacf0f500       10 minutes ago       Exited              kube-controller-manager   8                   eed392ff39793       kube-controller-manager-terramaster   kube-system
b77a47907a49c       a5f569d49a979       10 minutes ago       Exited              kube-apiserver            9                   4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
09bf9ad41956d       52546a367cc9e       2 hours ago          Running             coredns                   1                   606b7d83d3033       coredns-66bc5c9577-jktg5              kube-system
c77746adef30f       52546a367cc9e       2 hours ago          Running             coredns                   1                   def7113ff7975       coredns-66bc5c9577-4pfwh              kube-system
25eea9f3c6072       e83704a177312       2 hours ago          Running             kube-flannel              1                   61fe1ce47c3a5       kube-flannel-ds-476m6                 kube-flannel
4e9b9f21c43d4       e83704a177312       2 hours ago          Exited              install-cni               0                   61fe1ce47c3a5       kube-flannel-ds-476m6                 kube-flannel
1aeb73921171a       bb28ded63816e       2 hours ago          Exited              install-cni-plugin        1                   61fe1ce47c3a5       kube-flannel-ds-476m6                 kube-flannel
28ae8cdbab273       8aa150647e88a       2 hours ago          Running             kube-proxy                1                   1c12f453f8db7       kube-proxy-xvsmn                      kube-system
6f3f218580753       a3e246e9556e9       2 hours ago          Running             etcd                      1                   c849ee45d6d3b       etcd-terramaster                      kube-system
5e9f42fcfd0c0       e83704a177312       22 hours ago         Exited              kube-flannel              0                   19c4ca4f5c116       kube-flannel-ds-476m6                 kube-flannel
53022640fd832       52546a367cc9e       22 hours ago         Exited              coredns                   0                   0308b7aab5cff       coredns-66bc5c9577-4pfwh              kube-system
c85887a9bbc10       52546a367cc9e       22 hours ago         Exited              coredns                   0                   a28ad1013ff3a       coredns-66bc5c9577-jktg5              kube-system
43752398026bb       8aa150647e88a       22 hours ago         Exited              kube-proxy                0                   d793f19f16541       kube-proxy-xvsmn                      kube-system
e0b0ee109090c       a3e246e9556e9       22 hours ago         Exited              etcd                      0                   62c40297b515e       etcd-terramaster                      kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ kubectl get nodes
The connection to the server 192.168.150.131:6443 was refused - did you specify the right host or port?
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$ sudo crictl ps -a | grep kube-apiserver
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml"
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
320fdad12450a       a5f569d49a979       8 minutes ago       Exited              kube-apiserver            25                  4bb4e5f24bcec       kube-apiserver-terramaster            kube-system
master@terramaster:~/CIS_Kubernetes_Benchmark_V1.12.0/kube-bench$
